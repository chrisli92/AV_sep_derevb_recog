2022-10-30 17:26:03 [229 - INFO ] Create optimizer adam: {'lr': 0.001, 'weight_decay': 1e-05}
2022-10-30 17:26:03 [197 - INFO ] Model summary:
DervbNet(
  (df_computer): DFComputer(
    (stft): STFT(window=sqrt_hann, stride=256, requires_grad=False, kernel_size=514x512)
    (ln_LPS): ChannelWiseLayerNorm((257,), eps=1e-05, elementwise_affine=True)
  )
  (conv1x1_1): Conv1D(257, 256, kernel_size=(1,), stride=(1,))
  (lip_blocks): OxfordLipNet(
    (conv1d_blocks): Sequential(
      (0): OxfordLipConv1DBlock(
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (dconv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)
        (sconv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
      (1): OxfordLipConv1DBlock(
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (dconv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)
        (sconv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
      (2): OxfordLipConv1DBlock(
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (dconv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)
        (sconv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
      (3): OxfordLipConv1DBlock(
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (dconv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)
        (sconv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
      (4): OxfordLipConv1DBlock(
        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (prelu): PReLU(num_parameters=1)
        (dconv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256)
        (sconv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (audio_blocks): Sequential(
    (0): Sequential(
      (0): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (1): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (2): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (3): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (4): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (5): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (6): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (7): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (fusion_blocks): Sequential(
    (0): Sequential(
      (0): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (1): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (2): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (3): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (4): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (5): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (6): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (7): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
    )
    (1): Sequential(
      (0): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (1): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (2): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (3): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (4): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (5): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (6): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (7): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
    )
    (2): Sequential(
      (0): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (1): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(2,), dilation=(2,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (2): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(4,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (3): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(8,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (4): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(16,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (5): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(32,), dilation=(32,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (6): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(64,), dilation=(64,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
      (7): Conv1DBlock(
        (conv1x1): Conv1D(256, 512, kernel_size=(1,), stride=(1,))
        (prelu1): PReLU(num_parameters=1)
        (lnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dconv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(128,), dilation=(128,), groups=512)
        (prelu2): PReLU(num_parameters=1)
        (lnorm2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (sconv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
      )
    )
  )
  (conv1x1_2_real): Conv1D(256, 257, kernel_size=(1,), stride=(1,))
  (conv1x1_2_imag): Conv1D(256, 257, kernel_size=(1,), stride=(1,))
  (fc1): Linear(in_features=512, out_features=256, bias=True)
  (fc2): Linear(in_features=512, out_features=256, bias=True)
  (lfb): LFB()
  (istft): iSTFT(window=sqrt_hann, stride=256, requires_grad=False, kernel_size=514x512)
  (ln_visual_512): ChannelWiseLayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (ln_visual_256): ChannelWiseLayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
2022-10-30 17:26:03 [198 - INFO ] Loading model to GPUs:(0,), #param: 9.88M
2022-10-30 17:26:06 [307 - INFO ] START FROM EPOCH 0
2022-10-30 17:27:25 [265 - INFO ] Process 100-th batch and its stft mse: 4.024, sisnr: 2.224, lfb mse: 1.586, back loss is : 4.024 
2022-10-30 17:28:43 [265 - INFO ] Process 200-th batch and its stft mse: 4.084, sisnr: 1.785, lfb mse: 2.311, back loss is : 4.084 
2022-10-30 17:30:02 [265 - INFO ] Process 300-th batch and its stft mse: 3.690, sisnr: 3.073, lfb mse: 1.990, back loss is : 3.690 
2022-10-30 17:31:25 [265 - INFO ] Process 400-th batch and its stft mse: 3.740, sisnr: 3.043, lfb mse: 1.701, back loss is : 3.740 
2022-10-30 17:32:49 [265 - INFO ] Process 500-th batch and its stft mse: 3.476, sisnr: 2.389, lfb mse: 1.983, back loss is : 3.476 
2022-10-30 17:34:19 [265 - INFO ] Process 600-th batch and its stft mse: 3.636, sisnr: 3.192, lfb mse: 1.966, back loss is : 3.636 
2022-10-30 17:35:42 [265 - INFO ] Process 700-th batch and its stft mse: 3.709, sisnr: 2.973, lfb mse: 1.771, back loss is : 3.709 
2022-10-30 17:37:07 [265 - INFO ] Process 800-th batch and its stft mse: 3.857, sisnr: 3.551, lfb mse: 1.761, back loss is : 3.857 
2022-10-30 17:38:35 [265 - INFO ] Process 900-th batch and its stft mse: 4.308, sisnr: 3.527, lfb mse: 1.554, back loss is : 4.308 
2022-10-30 17:39:58 [265 - INFO ] Process 1000-th batch and its stft mse: 3.495, sisnr: 1.587, lfb mse: 2.121, back loss is : 3.495 
2022-10-30 17:41:23 [265 - INFO ] Process 1100-th batch and its stft mse: 3.387, sisnr: 1.880, lfb mse: 1.767, back loss is : 3.387 
2022-10-30 17:42:49 [265 - INFO ] Process 1200-th batch and its stft mse: 3.901, sisnr: 1.964, lfb mse: 1.600, back loss is : 3.901 
2022-10-30 17:44:18 [265 - INFO ] Process 1300-th batch and its stft mse: 3.836, sisnr: 1.691, lfb mse: 1.548, back loss is : 3.836 
2022-10-30 17:45:45 [265 - INFO ] Process 1400-th batch and its stft mse: 3.683, sisnr: 2.581, lfb mse: 1.830, back loss is : 3.683 
2022-10-30 17:47:17 [265 - INFO ] Process 1500-th batch and its stft mse: 4.258, sisnr: 3.247, lfb mse: 2.010, back loss is : 4.258 
2022-10-30 17:48:23 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  1: train = +4.03616(1294.22s/1531) | dev_loss = +12.70655, (43.17s/4272), sisnr= 10.04965, lfb= 1.33467, stft= 12.70655 | no impr, best = 10.00000
2022-10-30 17:49:50 [265 - INFO ] Process 100-th batch and its stft mse: 3.892, sisnr: 1.804, lfb mse: 2.139, back loss is : 3.892 
2022-10-30 17:51:07 [265 - INFO ] Process 200-th batch and its stft mse: 4.171, sisnr: 2.309, lfb mse: 2.287, back loss is : 4.171 
2022-10-30 17:52:26 [265 - INFO ] Process 300-th batch and its stft mse: 3.445, sisnr: 2.623, lfb mse: 2.028, back loss is : 3.445 
2022-10-30 17:53:49 [265 - INFO ] Process 400-th batch and its stft mse: 3.723, sisnr: 3.441, lfb mse: 2.019, back loss is : 3.723 
2022-10-30 17:55:12 [265 - INFO ] Process 500-th batch and its stft mse: 3.478, sisnr: 2.725, lfb mse: 1.845, back loss is : 3.478 
2022-10-30 17:56:35 [265 - INFO ] Process 600-th batch and its stft mse: 3.297, sisnr: 2.139, lfb mse: 2.130, back loss is : 3.297 
2022-10-30 17:57:56 [265 - INFO ] Process 700-th batch and its stft mse: 4.005, sisnr: 2.469, lfb mse: 1.816, back loss is : 4.005 
2022-10-30 17:59:23 [265 - INFO ] Process 800-th batch and its stft mse: 3.416, sisnr: 1.741, lfb mse: 1.669, back loss is : 3.416 
2022-10-30 18:00:52 [265 - INFO ] Process 900-th batch and its stft mse: 3.618, sisnr: 2.118, lfb mse: 1.819, back loss is : 3.618 
2022-10-30 18:02:15 [265 - INFO ] Process 1000-th batch and its stft mse: 3.759, sisnr: 2.188, lfb mse: 2.156, back loss is : 3.759 
2022-10-30 18:03:35 [265 - INFO ] Process 1100-th batch and its stft mse: 3.234, sisnr: 0.721, lfb mse: 1.903, back loss is : 3.234 
2022-10-30 18:04:55 [265 - INFO ] Process 1200-th batch and its stft mse: 3.196, sisnr: 1.082, lfb mse: 1.665, back loss is : 3.196 
2022-10-30 18:06:21 [265 - INFO ] Process 1300-th batch and its stft mse: 3.429, sisnr: 1.566, lfb mse: 2.415, back loss is : 3.429 
2022-10-30 18:07:44 [265 - INFO ] Process 1400-th batch and its stft mse: 3.947, sisnr: 3.318, lfb mse: 1.640, back loss is : 3.947 
2022-10-30 18:09:08 [265 - INFO ] Process 1500-th batch and its stft mse: 3.306, sisnr: 2.343, lfb mse: 1.751, back loss is : 3.306 
2022-10-30 18:10:09 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  2: train = +3.54343(1268.04s/1531) | dev_loss = +14.19551, (37.51s/4272), sisnr= 21.01831, lfb= 1.65549, stft= 14.19551 | no impr, best = 10.00000
2022-10-30 18:11:34 [265 - INFO ] Process 100-th batch and its stft mse: 3.970, sisnr: 2.725, lfb mse: 1.942, back loss is : 3.970 
2022-10-30 18:12:53 [265 - INFO ] Process 200-th batch and its stft mse: 3.971, sisnr: 3.027, lfb mse: 1.921, back loss is : 3.971 
2022-10-30 18:14:14 [265 - INFO ] Process 300-th batch and its stft mse: 2.911, sisnr: 0.732, lfb mse: 2.164, back loss is : 2.911 
2022-10-30 18:15:32 [265 - INFO ] Process 400-th batch and its stft mse: 3.221, sisnr: 1.544, lfb mse: 1.887, back loss is : 3.221 
2022-10-30 18:16:53 [265 - INFO ] Process 500-th batch and its stft mse: 3.714, sisnr: 2.171, lfb mse: 2.221, back loss is : 3.714 
2022-10-30 18:18:17 [265 - INFO ] Process 600-th batch and its stft mse: 3.110, sisnr: 1.973, lfb mse: 1.846, back loss is : 3.110 
2022-10-30 18:19:43 [265 - INFO ] Process 700-th batch and its stft mse: 3.345, sisnr: 1.607, lfb mse: 1.650, back loss is : 3.345 
2022-10-30 18:21:06 [265 - INFO ] Process 800-th batch and its stft mse: 3.157, sisnr: 2.584, lfb mse: 1.889, back loss is : 3.157 
2022-10-30 18:22:29 [265 - INFO ] Process 900-th batch and its stft mse: 4.293, sisnr: 2.945, lfb mse: 1.967, back loss is : 4.293 
2022-10-30 18:23:54 [265 - INFO ] Process 1000-th batch and its stft mse: 3.351, sisnr: 2.334, lfb mse: 2.019, back loss is : 3.351 
2022-10-30 18:25:23 [265 - INFO ] Process 1100-th batch and its stft mse: 3.594, sisnr: 2.602, lfb mse: 2.377, back loss is : 3.594 
2022-10-30 18:26:45 [265 - INFO ] Process 1200-th batch and its stft mse: 3.289, sisnr: 1.289, lfb mse: 1.482, back loss is : 3.289 
2022-10-30 18:28:12 [265 - INFO ] Process 1300-th batch and its stft mse: 4.102, sisnr: 2.442, lfb mse: 2.295, back loss is : 4.102 
2022-10-30 18:29:41 [265 - INFO ] Process 1400-th batch and its stft mse: 3.453, sisnr: 2.296, lfb mse: 1.881, back loss is : 3.453 
2022-10-30 18:31:08 [265 - INFO ] Process 1500-th batch and its stft mse: 3.489, sisnr: 1.539, lfb mse: 1.994, back loss is : 3.489 
2022-10-30 18:32:10 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  3: train = +3.49762(1281.90s/1531) | dev_loss = +10.28075, (39.39s/4272), sisnr= 10.45524, lfb= 1.32740, stft= 10.28075 | no impr, best = 10.00000
2022-10-30 18:33:34 [265 - INFO ] Process 100-th batch and its stft mse: 3.125, sisnr: 1.101, lfb mse: 1.563, back loss is : 3.125 
2022-10-30 18:35:00 [265 - INFO ] Process 200-th batch and its stft mse: 3.706, sisnr: 2.547, lfb mse: 2.297, back loss is : 3.706 
2022-10-30 18:36:22 [265 - INFO ] Process 300-th batch and its stft mse: 3.074, sisnr: 2.093, lfb mse: 2.042, back loss is : 3.074 
2022-10-30 18:37:45 [265 - INFO ] Process 400-th batch and its stft mse: 2.858, sisnr: 1.358, lfb mse: 1.641, back loss is : 2.858 
2022-10-30 18:39:08 [265 - INFO ] Process 500-th batch and its stft mse: 3.701, sisnr: 1.853, lfb mse: 2.047, back loss is : 3.701 
2022-10-30 18:40:30 [265 - INFO ] Process 600-th batch and its stft mse: 3.496, sisnr: 1.112, lfb mse: 1.890, back loss is : 3.496 
2022-10-30 18:41:48 [265 - INFO ] Process 700-th batch and its stft mse: 3.731, sisnr: 2.609, lfb mse: 1.946, back loss is : 3.731 
2022-10-30 18:43:12 [265 - INFO ] Process 800-th batch and its stft mse: 3.275, sisnr: 2.044, lfb mse: 2.328, back loss is : 3.275 
2022-10-30 18:44:33 [265 - INFO ] Process 900-th batch and its stft mse: 3.587, sisnr: 2.327, lfb mse: 1.553, back loss is : 3.587 
2022-10-30 18:45:55 [265 - INFO ] Process 1000-th batch and its stft mse: 3.713, sisnr: 1.563, lfb mse: 2.184, back loss is : 3.713 
2022-10-30 18:47:23 [265 - INFO ] Process 1100-th batch and its stft mse: 2.951, sisnr: 1.879, lfb mse: 1.894, back loss is : 2.951 
2022-10-30 18:48:41 [265 - INFO ] Process 1200-th batch and its stft mse: 4.031, sisnr: 1.580, lfb mse: 2.256, back loss is : 4.031 
2022-10-30 18:50:02 [265 - INFO ] Process 1300-th batch and its stft mse: 3.183, sisnr: 0.965, lfb mse: 1.812, back loss is : 3.183 
2022-10-30 18:51:29 [265 - INFO ] Process 1400-th batch and its stft mse: 3.138, sisnr: 1.804, lfb mse: 1.559, back loss is : 3.138 
2022-10-30 18:52:55 [265 - INFO ] Process 1500-th batch and its stft mse: 3.827, sisnr: 2.350, lfb mse: 2.376, back loss is : 3.827 
2022-10-30 18:54:14 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  4: train = +3.46519(1265.62s/1531) | dev_loss = +67.36782, (57.65s/4272), sisnr= 22.72728, lfb= 4.64299, stft= 67.36782 | no impr, best = 10.00000
2022-10-30 18:55:38 [265 - INFO ] Process 100-th batch and its stft mse: 3.624, sisnr: 1.919, lfb mse: 2.183, back loss is : 3.624 
2022-10-30 18:56:59 [265 - INFO ] Process 200-th batch and its stft mse: 3.501, sisnr: 1.599, lfb mse: 2.200, back loss is : 3.501 
2022-10-30 18:58:22 [265 - INFO ] Process 300-th batch and its stft mse: 3.034, sisnr: 2.081, lfb mse: 1.855, back loss is : 3.034 
2022-10-30 18:59:42 [265 - INFO ] Process 400-th batch and its stft mse: 3.109, sisnr: 2.014, lfb mse: 1.753, back loss is : 3.109 
2022-10-30 19:01:06 [265 - INFO ] Process 500-th batch and its stft mse: 3.732, sisnr: 1.916, lfb mse: 1.964, back loss is : 3.732 
2022-10-30 19:02:27 [265 - INFO ] Process 600-th batch and its stft mse: 3.139, sisnr: 1.438, lfb mse: 2.169, back loss is : 3.139 
2022-10-30 19:03:51 [265 - INFO ] Process 700-th batch and its stft mse: 2.914, sisnr: 1.083, lfb mse: 2.033, back loss is : 2.914 
2022-10-30 19:05:12 [265 - INFO ] Process 800-th batch and its stft mse: 3.370, sisnr: 1.671, lfb mse: 1.804, back loss is : 3.370 
2022-10-30 19:06:38 [265 - INFO ] Process 900-th batch and its stft mse: 3.722, sisnr: 1.609, lfb mse: 2.135, back loss is : 3.722 
2022-10-30 19:07:59 [265 - INFO ] Process 1000-th batch and its stft mse: 3.367, sisnr: 1.517, lfb mse: 1.876, back loss is : 3.367 
2022-10-30 19:09:23 [265 - INFO ] Process 1100-th batch and its stft mse: 3.590, sisnr: 1.985, lfb mse: 2.208, back loss is : 3.590 
2022-10-30 19:10:43 [265 - INFO ] Process 1200-th batch and its stft mse: 2.983, sisnr: 1.146, lfb mse: 1.948, back loss is : 2.983 
2022-10-30 19:12:07 [265 - INFO ] Process 1300-th batch and its stft mse: 3.968, sisnr: 2.357, lfb mse: 2.105, back loss is : 3.968 
2022-10-30 19:13:32 [265 - INFO ] Process 1400-th batch and its stft mse: 3.167, sisnr: 1.849, lfb mse: 2.170, back loss is : 3.167 
2022-10-30 19:14:50 [265 - INFO ] Process 1500-th batch and its stft mse: 3.686, sisnr: 2.402, lfb mse: 2.246, back loss is : 3.686 
2022-10-30 19:15:49 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  5: train = +3.43966(1259.51s/1531) | dev_loss = +3.94822, (34.87s/4272), sisnr= 2.58987, lfb= 1.44082, stft= 3.94822 
2022-10-30 19:17:15 [265 - INFO ] Process 100-th batch and its stft mse: 3.719, sisnr: 1.781, lfb mse: 1.968, back loss is : 3.719 
2022-10-30 19:18:37 [265 - INFO ] Process 200-th batch and its stft mse: 3.781, sisnr: 2.387, lfb mse: 2.173, back loss is : 3.781 
2022-10-30 19:19:55 [265 - INFO ] Process 300-th batch and its stft mse: 3.317, sisnr: 1.128, lfb mse: 1.612, back loss is : 3.317 
2022-10-30 19:21:18 [265 - INFO ] Process 400-th batch and its stft mse: 3.125, sisnr: 1.651, lfb mse: 1.813, back loss is : 3.125 
2022-10-30 19:22:37 [265 - INFO ] Process 500-th batch and its stft mse: 3.464, sisnr: 1.008, lfb mse: 1.782, back loss is : 3.464 
2022-10-30 19:23:55 [265 - INFO ] Process 600-th batch and its stft mse: 3.272, sisnr: 1.146, lfb mse: 1.877, back loss is : 3.272 
2022-10-30 19:25:14 [265 - INFO ] Process 700-th batch and its stft mse: 3.115, sisnr: 1.173, lfb mse: 2.259, back loss is : 3.115 
2022-10-30 19:26:35 [265 - INFO ] Process 800-th batch and its stft mse: 3.779, sisnr: 1.979, lfb mse: 1.942, back loss is : 3.779 
2022-10-30 19:27:56 [265 - INFO ] Process 900-th batch and its stft mse: 3.461, sisnr: 0.767, lfb mse: 1.795, back loss is : 3.461 
2022-10-30 19:29:22 [265 - INFO ] Process 1000-th batch and its stft mse: 3.233, sisnr: 1.430, lfb mse: 1.939, back loss is : 3.233 
2022-10-30 19:30:46 [265 - INFO ] Process 1100-th batch and its stft mse: 3.675, sisnr: 3.069, lfb mse: 2.014, back loss is : 3.675 
2022-10-30 19:32:02 [265 - INFO ] Process 1200-th batch and its stft mse: 3.552, sisnr: 1.379, lfb mse: 2.029, back loss is : 3.552 
2022-10-30 19:33:23 [265 - INFO ] Process 1300-th batch and its stft mse: 3.531, sisnr: 3.053, lfb mse: 2.194, back loss is : 3.531 
2022-10-30 19:34:47 [265 - INFO ] Process 1400-th batch and its stft mse: 3.594, sisnr: 2.798, lfb mse: 2.014, back loss is : 3.594 
2022-10-30 19:36:04 [265 - INFO ] Process 1500-th batch and its stft mse: 3.609, sisnr: 2.375, lfb mse: 1.869, back loss is : 3.609 
2022-10-30 19:37:04 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  6: train = +3.41470(1239.86s/1531) | dev_loss = +3.96975, (35.47s/4272), sisnr= 2.69315, lfb= 1.56899, stft= 3.96975 | no impr, best = 3.94822
2022-10-30 19:38:30 [265 - INFO ] Process 100-th batch and its stft mse: 3.527, sisnr: 1.605, lfb mse: 2.505, back loss is : 3.527 
2022-10-30 19:39:53 [265 - INFO ] Process 200-th batch and its stft mse: 3.739, sisnr: 2.518, lfb mse: 1.880, back loss is : 3.739 
2022-10-30 19:41:15 [265 - INFO ] Process 300-th batch and its stft mse: 3.313, sisnr: 1.011, lfb mse: 1.975, back loss is : 3.313 
2022-10-30 19:42:36 [265 - INFO ] Process 400-th batch and its stft mse: 2.950, sisnr: 0.667, lfb mse: 1.903, back loss is : 2.950 
2022-10-30 19:43:59 [265 - INFO ] Process 500-th batch and its stft mse: 3.353, sisnr: 1.650, lfb mse: 2.158, back loss is : 3.353 
2022-10-30 19:45:23 [265 - INFO ] Process 600-th batch and its stft mse: 3.312, sisnr: 1.163, lfb mse: 2.177, back loss is : 3.312 
2022-10-30 19:46:46 [265 - INFO ] Process 700-th batch and its stft mse: 3.178, sisnr: 0.471, lfb mse: 1.703, back loss is : 3.178 
2022-10-30 19:48:08 [265 - INFO ] Process 800-th batch and its stft mse: 3.436, sisnr: 1.123, lfb mse: 2.004, back loss is : 3.436 
2022-10-30 19:49:31 [265 - INFO ] Process 900-th batch and its stft mse: 3.522, sisnr: 0.680, lfb mse: 2.239, back loss is : 3.522 
2022-10-30 19:50:56 [265 - INFO ] Process 1000-th batch and its stft mse: 3.109, sisnr: 0.845, lfb mse: 2.112, back loss is : 3.109 
2022-10-30 19:52:21 [265 - INFO ] Process 1100-th batch and its stft mse: 2.904, sisnr: 0.920, lfb mse: 2.096, back loss is : 2.904 
2022-10-30 19:53:45 [265 - INFO ] Process 1200-th batch and its stft mse: 3.517, sisnr: 1.226, lfb mse: 2.357, back loss is : 3.517 
2022-10-30 19:55:06 [265 - INFO ] Process 1300-th batch and its stft mse: 3.545, sisnr: 2.161, lfb mse: 1.780, back loss is : 3.545 
2022-10-30 19:56:23 [265 - INFO ] Process 1400-th batch and its stft mse: 3.531, sisnr: 2.361, lfb mse: 1.902, back loss is : 3.531 
2022-10-30 19:57:44 [265 - INFO ] Process 1500-th batch and its stft mse: 3.534, sisnr: 2.452, lfb mse: 2.015, back loss is : 3.534 
2022-10-30 19:58:43 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  7: train = +3.39130(1262.61s/1531) | dev_loss = +3.59685, (35.85s/4272), sisnr= 1.57693, lfb= 1.19175, stft= 3.59685 
2022-10-30 20:00:08 [265 - INFO ] Process 100-th batch and its stft mse: 3.934, sisnr: 3.339, lfb mse: 2.116, back loss is : 3.934 
2022-10-30 20:01:28 [265 - INFO ] Process 200-th batch and its stft mse: 3.612, sisnr: 1.024, lfb mse: 2.413, back loss is : 3.612 
2022-10-30 20:02:53 [265 - INFO ] Process 300-th batch and its stft mse: 3.445, sisnr: 1.341, lfb mse: 2.418, back loss is : 3.445 
2022-10-30 20:04:15 [265 - INFO ] Process 400-th batch and its stft mse: 3.496, sisnr: 2.172, lfb mse: 2.306, back loss is : 3.496 
2022-10-30 20:05:37 [265 - INFO ] Process 500-th batch and its stft mse: 3.707, sisnr: 0.977, lfb mse: 2.532, back loss is : 3.707 
2022-10-30 20:06:58 [265 - INFO ] Process 600-th batch and its stft mse: 2.912, sisnr: 1.372, lfb mse: 2.113, back loss is : 2.912 
2022-10-30 20:08:16 [265 - INFO ] Process 700-th batch and its stft mse: 3.109, sisnr: 1.121, lfb mse: 1.976, back loss is : 3.109 
2022-10-30 20:09:37 [265 - INFO ] Process 800-th batch and its stft mse: 3.253, sisnr: 2.090, lfb mse: 1.974, back loss is : 3.253 
2022-10-30 20:11:00 [265 - INFO ] Process 900-th batch and its stft mse: 3.163, sisnr: 0.596, lfb mse: 2.148, back loss is : 3.163 
2022-10-30 20:12:25 [265 - INFO ] Process 1000-th batch and its stft mse: 3.676, sisnr: 1.849, lfb mse: 2.221, back loss is : 3.676 
2022-10-30 20:13:46 [265 - INFO ] Process 1100-th batch and its stft mse: 3.294, sisnr: 0.267, lfb mse: 2.088, back loss is : 3.294 
2022-10-30 20:15:06 [265 - INFO ] Process 1200-th batch and its stft mse: 3.149, sisnr: 0.651, lfb mse: 2.385, back loss is : 3.149 
2022-10-30 20:16:28 [265 - INFO ] Process 1300-th batch and its stft mse: 2.868, sisnr: 1.126, lfb mse: 2.045, back loss is : 2.868 
2022-10-30 20:17:44 [265 - INFO ] Process 1400-th batch and its stft mse: 3.549, sisnr: 1.600, lfb mse: 2.067, back loss is : 3.549 
2022-10-30 20:19:08 [265 - INFO ] Process 1500-th batch and its stft mse: 3.923, sisnr: 1.845, lfb mse: 1.858, back loss is : 3.923 
2022-10-30 20:20:09 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  8: train = +3.37191(1248.75s/1531) | dev_loss = +3.58520, (36.61s/4272), sisnr= 1.53964, lfb= 1.30399, stft= 3.58520 
2022-10-30 20:21:40 [265 - INFO ] Process 100-th batch and its stft mse: 3.544, sisnr: 1.996, lfb mse: 1.968, back loss is : 3.544 
2022-10-30 20:23:03 [265 - INFO ] Process 200-th batch and its stft mse: 3.307, sisnr: 1.477, lfb mse: 2.368, back loss is : 3.307 
2022-10-30 20:24:24 [265 - INFO ] Process 300-th batch and its stft mse: 2.606, sisnr: 0.896, lfb mse: 2.132, back loss is : 2.606 
2022-10-30 20:25:45 [265 - INFO ] Process 400-th batch and its stft mse: 3.646, sisnr: 1.903, lfb mse: 2.123, back loss is : 3.646 
2022-10-30 20:27:08 [265 - INFO ] Process 500-th batch and its stft mse: 3.140, sisnr: 0.819, lfb mse: 2.315, back loss is : 3.140 
2022-10-30 20:28:29 [265 - INFO ] Process 600-th batch and its stft mse: 3.072, sisnr: 1.367, lfb mse: 2.138, back loss is : 3.072 
2022-10-30 20:29:53 [265 - INFO ] Process 700-th batch and its stft mse: 3.566, sisnr: 1.008, lfb mse: 2.265, back loss is : 3.566 
2022-10-30 20:31:15 [265 - INFO ] Process 800-th batch and its stft mse: 3.343, sisnr: 2.034, lfb mse: 2.086, back loss is : 3.343 
2022-10-30 20:32:35 [265 - INFO ] Process 900-th batch and its stft mse: 3.046, sisnr: 0.881, lfb mse: 3.004, back loss is : 3.046 
2022-10-30 20:33:58 [265 - INFO ] Process 1000-th batch and its stft mse: 3.197, sisnr: 0.643, lfb mse: 2.041, back loss is : 3.197 
2022-10-30 20:35:18 [265 - INFO ] Process 1100-th batch and its stft mse: 3.754, sisnr: 2.429, lfb mse: 1.806, back loss is : 3.754 
2022-10-30 20:36:39 [265 - INFO ] Process 1200-th batch and its stft mse: 3.146, sisnr: 1.083, lfb mse: 2.230, back loss is : 3.146 
2022-10-30 20:37:57 [265 - INFO ] Process 1300-th batch and its stft mse: 3.203, sisnr: 2.058, lfb mse: 2.259, back loss is : 3.203 
2022-10-30 20:39:14 [265 - INFO ] Process 1400-th batch and its stft mse: 3.777, sisnr: 1.536, lfb mse: 2.469, back loss is : 3.777 
2022-10-30 20:40:38 [265 - INFO ] Process 1500-th batch and its stft mse: 3.340, sisnr: 3.078, lfb mse: 2.102, back loss is : 3.340 
2022-10-30 20:41:39 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch  9: train = +3.35025(1252.28s/1531) | dev_loss = +3.58021, (37.51s/4272), sisnr= 1.52779, lfb= 1.47284, stft= 3.58021 
2022-10-30 20:43:07 [265 - INFO ] Process 100-th batch and its stft mse: 3.687, sisnr: 1.716, lfb mse: 1.730, back loss is : 3.687 
2022-10-30 20:44:29 [265 - INFO ] Process 200-th batch and its stft mse: 3.348, sisnr: 3.566, lfb mse: 2.186, back loss is : 3.348 
2022-10-30 20:45:55 [265 - INFO ] Process 300-th batch and its stft mse: 3.496, sisnr: 1.520, lfb mse: 2.098, back loss is : 3.496 
2022-10-30 20:47:17 [265 - INFO ] Process 400-th batch and its stft mse: 3.055, sisnr: 1.893, lfb mse: 2.187, back loss is : 3.055 
2022-10-30 20:48:39 [265 - INFO ] Process 500-th batch and its stft mse: 3.406, sisnr: 0.778, lfb mse: 2.283, back loss is : 3.406 
2022-10-30 20:50:03 [265 - INFO ] Process 600-th batch and its stft mse: 3.425, sisnr: 1.730, lfb mse: 2.393, back loss is : 3.425 
2022-10-30 20:51:28 [265 - INFO ] Process 700-th batch and its stft mse: 3.007, sisnr: 0.967, lfb mse: 1.759, back loss is : 3.007 
2022-10-30 20:52:47 [265 - INFO ] Process 800-th batch and its stft mse: 3.581, sisnr: 0.621, lfb mse: 2.347, back loss is : 3.581 
2022-10-30 20:54:08 [265 - INFO ] Process 900-th batch and its stft mse: 3.557, sisnr: 1.141, lfb mse: 2.335, back loss is : 3.557 
2022-10-30 20:55:31 [265 - INFO ] Process 1000-th batch and its stft mse: 3.591, sisnr: 2.379, lfb mse: 2.124, back loss is : 3.591 
2022-10-30 20:56:54 [265 - INFO ] Process 1100-th batch and its stft mse: 4.089, sisnr: 2.735, lfb mse: 2.341, back loss is : 4.089 
2022-10-30 20:58:17 [265 - INFO ] Process 1200-th batch and its stft mse: 3.005, sisnr: 0.491, lfb mse: 2.197, back loss is : 3.005 
2022-10-30 20:59:32 [265 - INFO ] Process 1300-th batch and its stft mse: 3.149, sisnr: 1.220, lfb mse: 2.103, back loss is : 3.149 
2022-10-30 21:00:52 [265 - INFO ] Process 1400-th batch and its stft mse: 3.205, sisnr: 1.927, lfb mse: 2.096, back loss is : 3.205 
2022-10-30 21:02:18 [265 - INFO ] Process 1500-th batch and its stft mse: 3.722, sisnr: 3.064, lfb mse: 2.182, back loss is : 3.722 
2022-10-30 21:03:19 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 10: train = +3.33330(1264.37s/1531) | dev_loss = +3.57133, (34.99s/4272), sisnr= 1.47091, lfb= 1.20946, stft= 3.57133 
2022-10-30 21:04:45 [265 - INFO ] Process 100-th batch and its stft mse: 3.406, sisnr: 2.388, lfb mse: 2.242, back loss is : 3.406 
2022-10-30 21:06:04 [265 - INFO ] Process 200-th batch and its stft mse: 3.559, sisnr: 2.251, lfb mse: 2.265, back loss is : 3.559 
2022-10-30 21:07:29 [265 - INFO ] Process 300-th batch and its stft mse: 3.711, sisnr: 2.419, lfb mse: 2.275, back loss is : 3.711 
2022-10-30 21:08:48 [265 - INFO ] Process 400-th batch and its stft mse: 3.353, sisnr: 1.294, lfb mse: 1.797, back loss is : 3.353 
2022-10-30 21:10:15 [265 - INFO ] Process 500-th batch and its stft mse: 3.740, sisnr: 1.663, lfb mse: 2.305, back loss is : 3.740 
2022-10-30 21:11:36 [265 - INFO ] Process 600-th batch and its stft mse: 2.829, sisnr: 0.987, lfb mse: 2.380, back loss is : 2.829 
2022-10-30 21:13:00 [265 - INFO ] Process 700-th batch and its stft mse: 3.065, sisnr: 0.829, lfb mse: 2.394, back loss is : 3.065 
2022-10-30 21:14:25 [265 - INFO ] Process 800-th batch and its stft mse: 3.028, sisnr: 0.534, lfb mse: 2.167, back loss is : 3.028 
2022-10-30 21:15:50 [265 - INFO ] Process 900-th batch and its stft mse: 3.107, sisnr: 0.786, lfb mse: 2.720, back loss is : 3.107 
2022-10-30 21:17:12 [265 - INFO ] Process 1000-th batch and its stft mse: 3.382, sisnr: 1.214, lfb mse: 2.093, back loss is : 3.382 
2022-10-30 21:18:36 [265 - INFO ] Process 1100-th batch and its stft mse: 3.256, sisnr: 1.846, lfb mse: 2.396, back loss is : 3.256 
2022-10-30 21:19:55 [265 - INFO ] Process 1200-th batch and its stft mse: 3.557, sisnr: 0.621, lfb mse: 2.263, back loss is : 3.557 
2022-10-30 21:21:16 [265 - INFO ] Process 1300-th batch and its stft mse: 3.341, sisnr: 1.426, lfb mse: 2.668, back loss is : 3.341 
2022-10-30 21:22:38 [265 - INFO ] Process 1400-th batch and its stft mse: 3.771, sisnr: 2.436, lfb mse: 2.081, back loss is : 3.771 
2022-10-30 21:24:00 [265 - INFO ] Process 1500-th batch and its stft mse: 3.389, sisnr: 1.553, lfb mse: 1.897, back loss is : 3.389 
2022-10-30 21:25:02 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 11: train = +3.31637(1266.62s/1531) | dev_loss = +3.56407, (35.84s/4272), sisnr= 1.47993, lfb= 1.37393, stft= 3.56407 
2022-10-30 21:26:29 [265 - INFO ] Process 100-th batch and its stft mse: 3.014, sisnr: 0.954, lfb mse: 2.519, back loss is : 3.014 
2022-10-30 21:27:47 [265 - INFO ] Process 200-th batch and its stft mse: 3.251, sisnr: 2.224, lfb mse: 2.197, back loss is : 3.251 
2022-10-30 21:29:07 [265 - INFO ] Process 300-th batch and its stft mse: 3.092, sisnr: 1.433, lfb mse: 2.298, back loss is : 3.092 
2022-10-30 21:30:28 [265 - INFO ] Process 400-th batch and its stft mse: 2.888, sisnr: 0.451, lfb mse: 2.417, back loss is : 2.888 
2022-10-30 21:31:46 [265 - INFO ] Process 500-th batch and its stft mse: 3.454, sisnr: 2.138, lfb mse: 2.082, back loss is : 3.454 
2022-10-30 21:33:06 [265 - INFO ] Process 600-th batch and its stft mse: 3.428, sisnr: 1.498, lfb mse: 2.380, back loss is : 3.428 
2022-10-30 21:34:30 [265 - INFO ] Process 700-th batch and its stft mse: 3.151, sisnr: 0.958, lfb mse: 2.669, back loss is : 3.151 
2022-10-30 21:35:54 [265 - INFO ] Process 800-th batch and its stft mse: 3.262, sisnr: 0.847, lfb mse: 2.330, back loss is : 3.262 
2022-10-30 21:37:17 [265 - INFO ] Process 900-th batch and its stft mse: 3.455, sisnr: 1.862, lfb mse: 2.379, back loss is : 3.455 
2022-10-30 21:38:36 [265 - INFO ] Process 1000-th batch and its stft mse: 3.341, sisnr: 2.349, lfb mse: 2.041, back loss is : 3.341 
2022-10-30 21:39:57 [265 - INFO ] Process 1100-th batch and its stft mse: 3.434, sisnr: 1.212, lfb mse: 2.319, back loss is : 3.434 
2022-10-30 21:41:14 [265 - INFO ] Process 1200-th batch and its stft mse: 3.472, sisnr: 1.429, lfb mse: 2.467, back loss is : 3.472 
2022-10-30 21:42:34 [265 - INFO ] Process 1300-th batch and its stft mse: 3.547, sisnr: 2.086, lfb mse: 2.346, back loss is : 3.547 
2022-10-30 21:43:56 [265 - INFO ] Process 1400-th batch and its stft mse: 3.123, sisnr: 0.640, lfb mse: 2.154, back loss is : 3.123 
2022-10-30 21:45:16 [265 - INFO ] Process 1500-th batch and its stft mse: 3.609, sisnr: 0.615, lfb mse: 2.564, back loss is : 3.609 
2022-10-30 21:46:14 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 12: train = +3.29925(1236.97s/1531) | dev_loss = +3.56877, (34.78s/4272), sisnr= 1.49410, lfb= 1.35900, stft= 3.56877 | no impr, best = 3.56407
2022-10-30 21:47:40 [265 - INFO ] Process 100-th batch and its stft mse: 3.974, sisnr: 1.111, lfb mse: 2.986, back loss is : 3.974 
2022-10-30 21:49:00 [265 - INFO ] Process 200-th batch and its stft mse: 3.340, sisnr: 0.744, lfb mse: 2.538, back loss is : 3.340 
2022-10-30 21:50:20 [265 - INFO ] Process 300-th batch and its stft mse: 3.312, sisnr: 0.906, lfb mse: 1.769, back loss is : 3.312 
2022-10-30 21:51:41 [265 - INFO ] Process 400-th batch and its stft mse: 3.766, sisnr: 0.755, lfb mse: 2.514, back loss is : 3.766 
2022-10-30 21:53:02 [265 - INFO ] Process 500-th batch and its stft mse: 3.814, sisnr: 2.858, lfb mse: 2.264, back loss is : 3.814 
2022-10-30 21:54:22 [265 - INFO ] Process 600-th batch and its stft mse: 3.905, sisnr: 2.502, lfb mse: 2.800, back loss is : 3.905 
2022-10-30 21:55:42 [265 - INFO ] Process 700-th batch and its stft mse: 3.041, sisnr: 0.881, lfb mse: 2.401, back loss is : 3.041 
2022-10-30 21:57:04 [265 - INFO ] Process 800-th batch and its stft mse: 3.239, sisnr: 1.068, lfb mse: 1.863, back loss is : 3.239 
2022-10-30 21:58:22 [265 - INFO ] Process 900-th batch and its stft mse: 3.076, sisnr: 0.778, lfb mse: 2.560, back loss is : 3.076 
2022-10-30 21:59:43 [265 - INFO ] Process 1000-th batch and its stft mse: 2.934, sisnr: 0.968, lfb mse: 2.142, back loss is : 2.934 
2022-10-30 22:01:03 [265 - INFO ] Process 1100-th batch and its stft mse: 3.613, sisnr: 0.451, lfb mse: 2.073, back loss is : 3.613 
2022-10-30 22:02:21 [265 - INFO ] Process 1200-th batch and its stft mse: 3.861, sisnr: 2.457, lfb mse: 2.058, back loss is : 3.861 
2022-10-30 22:03:49 [265 - INFO ] Process 1300-th batch and its stft mse: 3.297, sisnr: 1.573, lfb mse: 2.306, back loss is : 3.297 
2022-10-30 22:05:11 [265 - INFO ] Process 1400-th batch and its stft mse: 3.454, sisnr: 1.750, lfb mse: 2.304, back loss is : 3.454 
2022-10-30 22:06:35 [265 - INFO ] Process 1500-th batch and its stft mse: 3.046, sisnr: 0.812, lfb mse: 1.846, back loss is : 3.046 
2022-10-30 22:07:37 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 13: train = +3.28394(1245.69s/1531) | dev_loss = +3.56505, (36.77s/4272), sisnr= 1.45599, lfb= 1.48781, stft= 3.56505 | no impr, best = 3.56407
2022-10-30 22:09:03 [265 - INFO ] Process 100-th batch and its stft mse: 2.744, sisnr: 0.574, lfb mse: 2.765, back loss is : 2.744 
2022-10-30 22:10:24 [265 - INFO ] Process 200-th batch and its stft mse: 3.809, sisnr: 1.049, lfb mse: 2.414, back loss is : 3.809 
2022-10-30 22:11:44 [265 - INFO ] Process 300-th batch and its stft mse: 3.303, sisnr: 1.805, lfb mse: 2.495, back loss is : 3.303 
2022-10-30 22:13:07 [265 - INFO ] Process 400-th batch and its stft mse: 3.014, sisnr: 1.123, lfb mse: 2.487, back loss is : 3.014 
2022-10-30 22:14:31 [265 - INFO ] Process 500-th batch and its stft mse: 3.351, sisnr: 1.708, lfb mse: 2.664, back loss is : 3.351 
2022-10-30 22:15:52 [265 - INFO ] Process 600-th batch and its stft mse: 3.754, sisnr: 2.225, lfb mse: 2.799, back loss is : 3.754 
2022-10-30 22:17:16 [265 - INFO ] Process 700-th batch and its stft mse: 3.169, sisnr: 1.690, lfb mse: 2.161, back loss is : 3.169 
2022-10-30 22:18:36 [265 - INFO ] Process 800-th batch and its stft mse: 3.710, sisnr: 3.275, lfb mse: 2.065, back loss is : 3.710 
2022-10-30 22:20:00 [265 - INFO ] Process 900-th batch and its stft mse: 3.153, sisnr: 1.174, lfb mse: 2.092, back loss is : 3.153 
2022-10-30 22:21:26 [265 - INFO ] Process 1000-th batch and its stft mse: 3.530, sisnr: 1.675, lfb mse: 2.388, back loss is : 3.530 
2022-10-30 22:22:47 [265 - INFO ] Process 1100-th batch and its stft mse: 3.087, sisnr: 0.424, lfb mse: 2.477, back loss is : 3.087 
2022-10-30 22:24:08 [265 - INFO ] Process 1200-th batch and its stft mse: 3.159, sisnr: 0.950, lfb mse: 2.289, back loss is : 3.159 
2022-10-30 22:25:34 [265 - INFO ] Process 1300-th batch and its stft mse: 3.294, sisnr: 0.670, lfb mse: 2.621, back loss is : 3.294 
2022-10-30 22:26:56 [265 - INFO ] Process 1400-th batch and its stft mse: 3.512, sisnr: 1.399, lfb mse: 2.357, back loss is : 3.512 
2022-10-30 22:28:16 [265 - INFO ] Process 1500-th batch and its stft mse: 3.280, sisnr: 1.496, lfb mse: 2.355, back loss is : 3.280 
2022-10-30 22:29:14 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 14: train = +3.26910(1263.11s/1531) | dev_loss = +3.55200, (34.37s/4272), sisnr= 1.41712, lfb= 1.28011, stft= 3.55200 
2022-10-30 22:30:43 [265 - INFO ] Process 100-th batch and its stft mse: 3.481, sisnr: 1.360, lfb mse: 2.589, back loss is : 3.481 
2022-10-30 22:32:05 [265 - INFO ] Process 200-th batch and its stft mse: 3.366, sisnr: 0.892, lfb mse: 2.762, back loss is : 3.366 
2022-10-30 22:33:24 [265 - INFO ] Process 300-th batch and its stft mse: 2.646, sisnr: 1.700, lfb mse: 2.407, back loss is : 2.646 
2022-10-30 22:34:43 [265 - INFO ] Process 400-th batch and its stft mse: 3.225, sisnr: 1.121, lfb mse: 2.483, back loss is : 3.225 
2022-10-30 22:36:04 [265 - INFO ] Process 500-th batch and its stft mse: 3.096, sisnr: 1.671, lfb mse: 2.869, back loss is : 3.096 
2022-10-30 22:37:22 [265 - INFO ] Process 600-th batch and its stft mse: 2.630, sisnr: 0.669, lfb mse: 2.182, back loss is : 2.630 
2022-10-30 22:38:46 [265 - INFO ] Process 700-th batch and its stft mse: 3.562, sisnr: 1.198, lfb mse: 2.237, back loss is : 3.562 
2022-10-30 22:40:10 [265 - INFO ] Process 800-th batch and its stft mse: 3.532, sisnr: 0.940, lfb mse: 2.240, back loss is : 3.532 
2022-10-30 22:41:31 [265 - INFO ] Process 900-th batch and its stft mse: 3.168, sisnr: 1.095, lfb mse: 2.241, back loss is : 3.168 
2022-10-30 22:42:51 [265 - INFO ] Process 1000-th batch and its stft mse: 3.572, sisnr: 1.792, lfb mse: 2.359, back loss is : 3.572 
2022-10-30 22:44:07 [265 - INFO ] Process 1100-th batch and its stft mse: 3.349, sisnr: 1.644, lfb mse: 2.617, back loss is : 3.349 
2022-10-30 22:45:30 [265 - INFO ] Process 1200-th batch and its stft mse: 3.134, sisnr: 1.685, lfb mse: 2.072, back loss is : 3.134 
2022-10-30 22:46:53 [265 - INFO ] Process 1300-th batch and its stft mse: 3.332, sisnr: 1.130, lfb mse: 2.354, back loss is : 3.332 
2022-10-30 22:48:16 [265 - INFO ] Process 1400-th batch and its stft mse: 3.325, sisnr: 1.965, lfb mse: 2.366, back loss is : 3.325 
2022-10-30 22:49:36 [265 - INFO ] Process 1500-th batch and its stft mse: 3.078, sisnr: 1.778, lfb mse: 2.273, back loss is : 3.078 
2022-10-30 22:50:35 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 15: train = +3.25833(1245.15s/1531) | dev_loss = +3.55723, (34.83s/4272), sisnr= 1.45924, lfb= 1.23624, stft= 3.55723 | no impr, best = 3.55200
2022-10-30 22:52:00 [265 - INFO ] Process 100-th batch and its stft mse: 3.610, sisnr: 1.457, lfb mse: 2.627, back loss is : 3.610 
2022-10-30 22:53:20 [265 - INFO ] Process 200-th batch and its stft mse: 3.014, sisnr: 0.911, lfb mse: 2.174, back loss is : 3.014 
2022-10-30 22:54:41 [265 - INFO ] Process 300-th batch and its stft mse: 3.160, sisnr: 0.672, lfb mse: 2.386, back loss is : 3.160 
2022-10-30 22:56:03 [265 - INFO ] Process 400-th batch and its stft mse: 2.937, sisnr: 0.538, lfb mse: 3.009, back loss is : 2.937 
2022-10-30 22:57:26 [265 - INFO ] Process 500-th batch and its stft mse: 3.132, sisnr: 2.099, lfb mse: 2.121, back loss is : 3.132 
2022-10-30 22:58:51 [265 - INFO ] Process 600-th batch and its stft mse: 3.823, sisnr: 2.025, lfb mse: 2.686, back loss is : 3.823 
2022-10-30 23:00:15 [265 - INFO ] Process 700-th batch and its stft mse: 3.406, sisnr: 2.055, lfb mse: 2.625, back loss is : 3.406 
2022-10-30 23:01:37 [265 - INFO ] Process 800-th batch and its stft mse: 2.805, sisnr: 0.777, lfb mse: 2.566, back loss is : 2.805 
2022-10-30 23:03:01 [265 - INFO ] Process 900-th batch and its stft mse: 3.412, sisnr: 1.998, lfb mse: 2.649, back loss is : 3.412 
2022-10-30 23:04:19 [265 - INFO ] Process 1000-th batch and its stft mse: 3.218, sisnr: 1.151, lfb mse: 2.443, back loss is : 3.218 
2022-10-30 23:05:38 [265 - INFO ] Process 1100-th batch and its stft mse: 2.711, sisnr: 0.210, lfb mse: 2.278, back loss is : 2.711 
2022-10-30 23:06:56 [265 - INFO ] Process 1200-th batch and its stft mse: 3.157, sisnr: 1.184, lfb mse: 2.567, back loss is : 3.157 
2022-10-30 23:08:17 [265 - INFO ] Process 1300-th batch and its stft mse: 3.093, sisnr: 1.275, lfb mse: 2.476, back loss is : 3.093 
2022-10-30 23:09:36 [265 - INFO ] Process 1400-th batch and its stft mse: 3.103, sisnr: 2.703, lfb mse: 2.254, back loss is : 3.103 
2022-10-30 23:10:55 [265 - INFO ] Process 1500-th batch and its stft mse: 2.857, sisnr: 1.415, lfb mse: 2.063, back loss is : 2.857 
2022-10-30 23:11:57 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 16: train = +3.24423(1245.74s/1531) | dev_loss = +3.55720, (36.08s/4272), sisnr= 1.42283, lfb= 1.37806, stft= 3.55720 | no impr, best = 3.55200
2022-10-30 23:13:24 [265 - INFO ] Process 100-th batch and its stft mse: 3.101, sisnr: 1.752, lfb mse: 2.629, back loss is : 3.101 
2022-10-30 23:14:46 [265 - INFO ] Process 200-th batch and its stft mse: 3.767, sisnr: 1.511, lfb mse: 2.179, back loss is : 3.767 
2022-10-30 23:16:08 [265 - INFO ] Process 300-th batch and its stft mse: 3.619, sisnr: 1.097, lfb mse: 2.567, back loss is : 3.619 
2022-10-30 23:17:34 [265 - INFO ] Process 400-th batch and its stft mse: 3.003, sisnr: 1.108, lfb mse: 2.314, back loss is : 3.003 
2022-10-30 23:18:54 [265 - INFO ] Process 500-th batch and its stft mse: 3.499, sisnr: 0.084, lfb mse: 2.332, back loss is : 3.499 
2022-10-30 23:20:15 [265 - INFO ] Process 600-th batch and its stft mse: 3.618, sisnr: 1.543, lfb mse: 2.243, back loss is : 3.618 
2022-10-30 23:21:40 [265 - INFO ] Process 700-th batch and its stft mse: 2.913, sisnr: 0.246, lfb mse: 2.297, back loss is : 2.913 
2022-10-30 23:22:59 [265 - INFO ] Process 800-th batch and its stft mse: 3.573, sisnr: 1.602, lfb mse: 2.282, back loss is : 3.573 
2022-10-30 23:24:21 [265 - INFO ] Process 900-th batch and its stft mse: 2.717, sisnr: 0.942, lfb mse: 2.076, back loss is : 2.717 
2022-10-30 23:25:36 [265 - INFO ] Process 1000-th batch and its stft mse: 3.399, sisnr: 1.368, lfb mse: 2.484, back loss is : 3.399 
2022-10-30 23:27:00 [265 - INFO ] Process 1100-th batch and its stft mse: 2.929, sisnr: 0.974, lfb mse: 2.436, back loss is : 2.929 
2022-10-30 23:28:25 [265 - INFO ] Process 1200-th batch and its stft mse: 3.384, sisnr: 2.450, lfb mse: 2.712, back loss is : 3.384 
2022-10-30 23:29:47 [265 - INFO ] Process 1300-th batch and its stft mse: 3.251, sisnr: 2.001, lfb mse: 2.608, back loss is : 3.251 
2022-10-30 23:31:08 [265 - INFO ] Process 1400-th batch and its stft mse: 3.202, sisnr: 0.302, lfb mse: 2.183, back loss is : 3.202 
2022-10-30 23:32:33 [265 - INFO ] Process 1500-th batch and its stft mse: 3.475, sisnr: 2.043, lfb mse: 2.134, back loss is : 3.475 
2022-10-30 23:33:33 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 17: train = +3.23469(1260.67s/1531) | dev_loss = +3.56024, (35.09s/4272), sisnr= 1.47717, lfb= 1.37030, stft= 3.56024 | no impr, best = 3.55200
2022-10-30 23:34:59 [265 - INFO ] Process 100-th batch and its stft mse: 3.090, sisnr: 0.957, lfb mse: 2.449, back loss is : 3.090 
2022-10-30 23:36:21 [265 - INFO ] Process 200-th batch and its stft mse: 3.184, sisnr: 0.914, lfb mse: 2.213, back loss is : 3.184 
2022-10-30 23:37:46 [265 - INFO ] Process 300-th batch and its stft mse: 3.772, sisnr: 1.368, lfb mse: 3.557, back loss is : 3.772 
2022-10-30 23:39:09 [265 - INFO ] Process 400-th batch and its stft mse: 3.385, sisnr: 0.484, lfb mse: 2.357, back loss is : 3.385 
2022-10-30 23:40:27 [265 - INFO ] Process 500-th batch and its stft mse: 2.950, sisnr: 0.489, lfb mse: 2.509, back loss is : 2.950 
2022-10-30 23:41:50 [265 - INFO ] Process 600-th batch and its stft mse: 3.707, sisnr: 1.631, lfb mse: 2.248, back loss is : 3.707 
2022-10-30 23:43:11 [265 - INFO ] Process 700-th batch and its stft mse: 3.142, sisnr: 1.561, lfb mse: 2.172, back loss is : 3.142 
2022-10-30 23:44:29 [265 - INFO ] Process 800-th batch and its stft mse: 3.225, sisnr: 1.573, lfb mse: 2.310, back loss is : 3.225 
2022-10-30 23:45:51 [265 - INFO ] Process 900-th batch and its stft mse: 3.598, sisnr: 1.342, lfb mse: 2.331, back loss is : 3.598 
2022-10-30 23:47:11 [265 - INFO ] Process 1000-th batch and its stft mse: 3.529, sisnr: 1.289, lfb mse: 2.032, back loss is : 3.529 
2022-10-30 23:48:31 [265 - INFO ] Process 1100-th batch and its stft mse: 3.289, sisnr: 0.117, lfb mse: 2.539, back loss is : 3.289 
2022-10-30 23:49:55 [265 - INFO ] Process 1200-th batch and its stft mse: 3.278, sisnr: 2.468, lfb mse: 2.328, back loss is : 3.278 
2022-10-30 23:51:16 [265 - INFO ] Process 1300-th batch and its stft mse: 3.159, sisnr: 1.377, lfb mse: 2.198, back loss is : 3.159 
2022-10-30 23:52:38 [265 - INFO ] Process 1400-th batch and its stft mse: 3.459, sisnr: 0.498, lfb mse: 2.165, back loss is : 3.459 
2022-10-30 23:54:00 [265 - INFO ] Process 1500-th batch and its stft mse: 3.443, sisnr: 1.227, lfb mse: 2.443, back loss is : 3.443 
2022-10-30 23:54:59 [331 - INFO ] Loss(time/N, lr=1.000e-03) - Epoch 18: train = +3.22496(1251.23s/1531) | dev_loss = +3.56203, (34.88s/4272), sisnr= 1.38767, lfb= 1.26436, stft= 3.56203 | no impr, best = 3.55200
2022-10-30 23:56:28 [265 - INFO ] Process 100-th batch and its stft mse: 3.392, sisnr: 1.204, lfb mse: 2.323, back loss is : 3.392 
2022-10-30 23:57:49 [265 - INFO ] Process 200-th batch and its stft mse: 2.610, sisnr: -0.175, lfb mse: 2.475, back loss is : 2.610 
2022-10-30 23:59:14 [265 - INFO ] Process 300-th batch and its stft mse: 3.003, sisnr: 0.340, lfb mse: 2.278, back loss is : 3.003 
2022-10-31 00:00:38 [265 - INFO ] Process 400-th batch and its stft mse: 2.694, sisnr: 0.422, lfb mse: 2.793, back loss is : 2.694 
2022-10-31 00:01:58 [265 - INFO ] Process 500-th batch and its stft mse: 2.795, sisnr: 1.579, lfb mse: 2.461, back loss is : 2.795 
2022-10-31 00:03:17 [265 - INFO ] Process 600-th batch and its stft mse: 3.172, sisnr: 1.023, lfb mse: 2.466, back loss is : 3.172 
2022-10-31 00:04:39 [265 - INFO ] Process 700-th batch and its stft mse: 3.005, sisnr: 0.289, lfb mse: 2.127, back loss is : 3.005 
2022-10-31 00:06:01 [265 - INFO ] Process 800-th batch and its stft mse: 2.808, sisnr: 1.151, lfb mse: 2.667, back loss is : 2.808 
2022-10-31 00:07:19 [265 - INFO ] Process 900-th batch and its stft mse: 3.135, sisnr: 1.440, lfb mse: 2.172, back loss is : 3.135 
2022-10-31 00:08:38 [265 - INFO ] Process 1000-th batch and its stft mse: 3.323, sisnr: 0.720, lfb mse: 2.795, back loss is : 3.323 
2022-10-31 00:09:58 [265 - INFO ] Process 1100-th batch and its stft mse: 2.573, sisnr: 0.119, lfb mse: 2.096, back loss is : 2.573 
2022-10-31 00:11:20 [265 - INFO ] Process 1200-th batch and its stft mse: 3.588, sisnr: 0.837, lfb mse: 2.428, back loss is : 3.588 
2022-10-31 00:12:42 [265 - INFO ] Process 1300-th batch and its stft mse: 3.750, sisnr: 1.150, lfb mse: 2.627, back loss is : 3.750 
2022-10-31 00:14:03 [265 - INFO ] Process 1400-th batch and its stft mse: 3.428, sisnr: 1.233, lfb mse: 2.654, back loss is : 3.428 
2022-10-31 00:15:26 [265 - INFO ] Process 1500-th batch and its stft mse: 3.414, sisnr: 1.105, lfb mse: 2.271, back loss is : 3.414 
2022-10-31 00:16:25 [331 - INFO ] Loss(time/N, lr=5.000e-04) - Epoch 19: train = +3.15313(1249.36s/1531) | dev_loss = +3.52913, (35.87s/4272), sisnr= 1.34854, lfb= 1.43632, stft= 3.52913 
2022-10-31 00:17:53 [265 - INFO ] Process 100-th batch and its stft mse: 2.960, sisnr: 0.668, lfb mse: 2.445, back loss is : 2.960 
2022-10-31 00:19:11 [265 - INFO ] Process 200-th batch and its stft mse: 3.202, sisnr: 1.598, lfb mse: 2.663, back loss is : 3.202 
2022-10-31 00:20:34 [265 - INFO ] Process 300-th batch and its stft mse: 3.359, sisnr: 1.061, lfb mse: 2.589, back loss is : 3.359 
2022-10-31 00:21:57 [265 - INFO ] Process 400-th batch and its stft mse: 3.168, sisnr: 1.180, lfb mse: 2.899, back loss is : 3.168 
2022-10-31 00:23:18 [265 - INFO ] Process 500-th batch and its stft mse: 2.718, sisnr: 1.427, lfb mse: 2.455, back loss is : 2.718 
2022-10-31 00:24:39 [265 - INFO ] Process 600-th batch and its stft mse: 2.853, sisnr: 0.312, lfb mse: 2.197, back loss is : 2.853 
2022-10-31 00:26:00 [265 - INFO ] Process 700-th batch and its stft mse: 3.148, sisnr: 0.895, lfb mse: 2.334, back loss is : 3.148 
2022-10-31 00:27:22 [265 - INFO ] Process 800-th batch and its stft mse: 2.850, sisnr: 0.259, lfb mse: 2.616, back loss is : 2.850 
2022-10-31 00:28:44 [265 - INFO ] Process 900-th batch and its stft mse: 2.855, sisnr: 0.107, lfb mse: 2.472, back loss is : 2.855 
2022-10-31 00:30:03 [265 - INFO ] Process 1000-th batch and its stft mse: 2.563, sisnr: 0.328, lfb mse: 2.405, back loss is : 2.563 
2022-10-31 00:31:24 [265 - INFO ] Process 1100-th batch and its stft mse: 3.362, sisnr: 0.584, lfb mse: 2.412, back loss is : 3.362 
2022-10-31 00:32:47 [265 - INFO ] Process 1200-th batch and its stft mse: 2.883, sisnr: 0.101, lfb mse: 2.887, back loss is : 2.883 
2022-10-31 00:34:10 [265 - INFO ] Process 1300-th batch and its stft mse: 3.089, sisnr: 0.288, lfb mse: 2.508, back loss is : 3.089 
2022-10-31 00:35:31 [265 - INFO ] Process 1400-th batch and its stft mse: 3.360, sisnr: 1.422, lfb mse: 2.309, back loss is : 3.360 
2022-10-31 00:36:55 [265 - INFO ] Process 1500-th batch and its stft mse: 2.853, sisnr: 0.748, lfb mse: 2.485, back loss is : 2.853 
2022-10-31 00:37:55 [331 - INFO ] Loss(time/N, lr=5.000e-04) - Epoch 20: train = +3.11756(1254.97s/1531) | dev_loss = +3.54072, (35.42s/4272), sisnr= 1.36104, lfb= 1.47390, stft= 3.54072 | no impr, best = 3.52913
2022-10-31 00:39:26 [265 - INFO ] Process 100-th batch and its stft mse: 2.796, sisnr: 0.524, lfb mse: 2.323, back loss is : 2.796 
2022-10-31 00:40:47 [265 - INFO ] Process 200-th batch and its stft mse: 3.178, sisnr: 0.222, lfb mse: 2.497, back loss is : 3.178 
2022-10-31 00:42:10 [265 - INFO ] Process 300-th batch and its stft mse: 2.853, sisnr: 1.005, lfb mse: 1.860, back loss is : 2.853 
2022-10-31 00:43:30 [265 - INFO ] Process 400-th batch and its stft mse: 2.648, sisnr: -0.025, lfb mse: 2.335, back loss is : 2.648 
2022-10-31 00:44:51 [265 - INFO ] Process 500-th batch and its stft mse: 2.860, sisnr: 0.607, lfb mse: 2.259, back loss is : 2.860 
2022-10-31 00:46:14 [265 - INFO ] Process 600-th batch and its stft mse: 3.091, sisnr: 0.725, lfb mse: 2.671, back loss is : 3.091 
2022-10-31 00:47:38 [265 - INFO ] Process 700-th batch and its stft mse: 3.097, sisnr: 1.708, lfb mse: 2.929, back loss is : 3.097 
2022-10-31 00:48:56 [265 - INFO ] Process 800-th batch and its stft mse: 2.890, sisnr: 0.651, lfb mse: 2.943, back loss is : 2.890 
2022-10-31 00:50:19 [265 - INFO ] Process 900-th batch and its stft mse: 2.888, sisnr: 0.387, lfb mse: 2.348, back loss is : 2.888 
2022-10-31 00:51:36 [265 - INFO ] Process 1000-th batch and its stft mse: 3.283, sisnr: 0.838, lfb mse: 2.853, back loss is : 3.283 
2022-10-31 00:52:53 [265 - INFO ] Process 1100-th batch and its stft mse: 2.902, sisnr: 0.056, lfb mse: 2.088, back loss is : 2.902 
2022-10-31 00:54:17 [265 - INFO ] Process 1200-th batch and its stft mse: 3.495, sisnr: 0.732, lfb mse: 2.331, back loss is : 3.495 
2022-10-31 00:55:40 [265 - INFO ] Process 1300-th batch and its stft mse: 3.230, sisnr: 1.119, lfb mse: 2.106, back loss is : 3.230 
2022-10-31 00:57:03 [265 - INFO ] Process 1400-th batch and its stft mse: 2.810, sisnr: 0.604, lfb mse: 2.649, back loss is : 2.810 
2022-10-31 00:58:29 [265 - INFO ] Process 1500-th batch and its stft mse: 3.188, sisnr: 1.488, lfb mse: 2.540, back loss is : 3.188 
2022-10-31 00:59:28 [331 - INFO ] Loss(time/N, lr=5.000e-04) - Epoch 21: train = +3.09946(1255.22s/1531) | dev_loss = +3.54056, (37.52s/4272), sisnr= 1.40504, lfb= 1.56758, stft= 3.54056 | no impr, best = 3.52913
2022-10-31 01:00:55 [265 - INFO ] Process 100-th batch and its stft mse: 2.986, sisnr: 0.469, lfb mse: 2.982, back loss is : 2.986 
2022-10-31 01:02:17 [265 - INFO ] Process 200-th batch and its stft mse: 2.839, sisnr: 0.498, lfb mse: 2.365, back loss is : 2.839 
2022-10-31 01:03:39 [265 - INFO ] Process 300-th batch and its stft mse: 3.240, sisnr: 1.359, lfb mse: 2.478, back loss is : 3.240 
2022-10-31 01:04:59 [265 - INFO ] Process 400-th batch and its stft mse: 2.769, sisnr: 0.312, lfb mse: 2.898, back loss is : 2.769 
2022-10-31 01:06:24 [265 - INFO ] Process 500-th batch and its stft mse: 3.194, sisnr: 1.092, lfb mse: 2.418, back loss is : 3.194 
2022-10-31 01:07:44 [265 - INFO ] Process 600-th batch and its stft mse: 2.912, sisnr: -0.377, lfb mse: 3.415, back loss is : 2.912 
2022-10-31 01:09:01 [265 - INFO ] Process 700-th batch and its stft mse: 3.165, sisnr: 0.390, lfb mse: 2.630, back loss is : 3.165 
2022-10-31 01:10:22 [265 - INFO ] Process 800-th batch and its stft mse: 2.957, sisnr: -0.238, lfb mse: 2.328, back loss is : 2.957 
2022-10-31 01:11:43 [265 - INFO ] Process 900-th batch and its stft mse: 2.634, sisnr: -0.132, lfb mse: 2.469, back loss is : 2.634 
2022-10-31 01:13:03 [265 - INFO ] Process 1000-th batch and its stft mse: 2.926, sisnr: 1.661, lfb mse: 3.070, back loss is : 2.926 
2022-10-31 01:14:28 [265 - INFO ] Process 1100-th batch and its stft mse: 3.237, sisnr: 1.014, lfb mse: 2.518, back loss is : 3.237 
2022-10-31 01:15:55 [265 - INFO ] Process 1200-th batch and its stft mse: 2.782, sisnr: -0.150, lfb mse: 2.088, back loss is : 2.782 
2022-10-31 01:17:14 [265 - INFO ] Process 1300-th batch and its stft mse: 3.368, sisnr: 1.133, lfb mse: 2.801, back loss is : 3.368 
2022-10-31 01:18:38 [265 - INFO ] Process 1400-th batch and its stft mse: 3.144, sisnr: 0.877, lfb mse: 2.831, back loss is : 3.144 
2022-10-31 01:19:59 [265 - INFO ] Process 1500-th batch and its stft mse: 3.140, sisnr: 0.805, lfb mse: 2.444, back loss is : 3.140 
2022-10-31 01:21:02 [331 - INFO ] Loss(time/N, lr=5.000e-04) - Epoch 22: train = +3.08420(1255.36s/1531) | dev_loss = +3.54799, (37.95s/4272), sisnr= 1.45090, lfb= 1.70322, stft= 3.54799 | no impr, best = 3.52913
2022-10-31 01:22:29 [265 - INFO ] Process 100-th batch and its stft mse: 2.836, sisnr: -0.304, lfb mse: 2.827, back loss is : 2.836 
2022-10-31 01:23:52 [265 - INFO ] Process 200-th batch and its stft mse: 3.343, sisnr: 0.893, lfb mse: 2.974, back loss is : 3.343 
2022-10-31 01:25:13 [265 - INFO ] Process 300-th batch and its stft mse: 3.073, sisnr: 0.218, lfb mse: 2.650, back loss is : 3.073 
2022-10-31 01:26:38 [265 - INFO ] Process 400-th batch and its stft mse: 2.763, sisnr: -0.247, lfb mse: 3.278, back loss is : 2.763 
2022-10-31 01:28:00 [265 - INFO ] Process 500-th batch and its stft mse: 3.396, sisnr: 1.551, lfb mse: 2.238, back loss is : 3.396 
2022-10-31 01:29:20 [265 - INFO ] Process 600-th batch and its stft mse: 3.095, sisnr: 0.790, lfb mse: 2.149, back loss is : 3.095 
2022-10-31 01:30:43 [265 - INFO ] Process 700-th batch and its stft mse: 2.434, sisnr: -0.224, lfb mse: 2.458, back loss is : 2.434 
2022-10-31 01:32:05 [265 - INFO ] Process 800-th batch and its stft mse: 2.991, sisnr: 1.307, lfb mse: 2.364, back loss is : 2.991 
2022-10-31 01:33:26 [265 - INFO ] Process 900-th batch and its stft mse: 3.155, sisnr: 0.026, lfb mse: 2.680, back loss is : 3.155 
2022-10-31 01:34:48 [265 - INFO ] Process 1000-th batch and its stft mse: 3.030, sisnr: -0.193, lfb mse: 2.339, back loss is : 3.030 
2022-10-31 01:36:11 [265 - INFO ] Process 1100-th batch and its stft mse: 3.313, sisnr: 1.095, lfb mse: 2.806, back loss is : 3.313 
2022-10-31 01:37:36 [265 - INFO ] Process 1200-th batch and its stft mse: 3.425, sisnr: 1.144, lfb mse: 2.298, back loss is : 3.425 
2022-10-31 01:38:56 [265 - INFO ] Process 1300-th batch and its stft mse: 2.861, sisnr: 0.950, lfb mse: 2.674, back loss is : 2.861 
2022-10-31 01:40:18 [265 - INFO ] Process 1400-th batch and its stft mse: 3.116, sisnr: 0.417, lfb mse: 1.977, back loss is : 3.116 
2022-10-31 01:41:38 [265 - INFO ] Process 1500-th batch and its stft mse: 2.922, sisnr: 0.551, lfb mse: 2.756, back loss is : 2.922 
2022-10-31 01:42:39 [331 - INFO ] Loss(time/N, lr=5.000e-04) - Epoch 23: train = +3.07264(1259.94s/1531) | dev_loss = +3.56143, (36.31s/4272), sisnr= 1.46551, lfb= 1.62248, stft= 3.56143 | no impr, best = 3.52913
2022-10-31 01:44:05 [265 - INFO ] Process 100-th batch and its stft mse: 2.755, sisnr: 0.685, lfb mse: 3.208, back loss is : 2.755 
2022-10-31 01:45:26 [265 - INFO ] Process 200-th batch and its stft mse: 2.852, sisnr: -0.070, lfb mse: 2.478, back loss is : 2.852 
2022-10-31 01:46:47 [265 - INFO ] Process 300-th batch and its stft mse: 3.224, sisnr: 0.887, lfb mse: 1.965, back loss is : 3.224 
2022-10-31 01:48:11 [265 - INFO ] Process 400-th batch and its stft mse: 2.802, sisnr: 0.434, lfb mse: 3.058, back loss is : 2.802 
2022-10-31 01:49:25 [265 - INFO ] Process 500-th batch and its stft mse: 2.858, sisnr: 0.672, lfb mse: 3.157, back loss is : 2.858 
2022-10-31 01:50:44 [265 - INFO ] Process 600-th batch and its stft mse: 2.773, sisnr: 0.631, lfb mse: 2.609, back loss is : 2.773 
2022-10-31 01:52:05 [265 - INFO ] Process 700-th batch and its stft mse: 3.082, sisnr: -0.238, lfb mse: 2.463, back loss is : 3.082 
2022-10-31 01:53:27 [265 - INFO ] Process 800-th batch and its stft mse: 2.684, sisnr: 0.699, lfb mse: 2.758, back loss is : 2.684 
2022-10-31 01:54:50 [265 - INFO ] Process 900-th batch and its stft mse: 3.143, sisnr: 0.693, lfb mse: 3.259, back loss is : 3.143 
2022-10-31 01:56:12 [265 - INFO ] Process 1000-th batch and its stft mse: 3.263, sisnr: 1.135, lfb mse: 2.765, back loss is : 3.263 
2022-10-31 01:57:32 [265 - INFO ] Process 1100-th batch and its stft mse: 2.603, sisnr: 0.641, lfb mse: 2.567, back loss is : 2.603 
2022-10-31 01:58:51 [265 - INFO ] Process 1200-th batch and its stft mse: 3.253, sisnr: 0.352, lfb mse: 2.164, back loss is : 3.253 
2022-10-31 02:00:11 [265 - INFO ] Process 1300-th batch and its stft mse: 2.970, sisnr: -0.624, lfb mse: 2.289, back loss is : 2.970 
2022-10-31 02:01:35 [265 - INFO ] Process 1400-th batch and its stft mse: 3.228, sisnr: 0.940, lfb mse: 3.188, back loss is : 3.228 
2022-10-31 02:02:58 [265 - INFO ] Process 1500-th batch and its stft mse: 2.656, sisnr: -0.714, lfb mse: 2.596, back loss is : 2.656 
2022-10-31 02:03:56 [331 - INFO ] Loss(time/N, lr=2.500e-04) - Epoch 24: train = +3.01855(1242.18s/1531) | dev_loss = +3.55021, (35.43s/4272), sisnr= 1.42455, lfb= 1.61451, stft= 3.55021 | no impr, best = 3.52913
2022-10-31 02:05:26 [265 - INFO ] Process 100-th batch and its stft mse: 2.998, sisnr: 0.137, lfb mse: 2.550, back loss is : 2.998 
2022-10-31 02:06:49 [265 - INFO ] Process 200-th batch and its stft mse: 2.890, sisnr: 0.669, lfb mse: 2.220, back loss is : 2.890 
2022-10-31 02:08:12 [265 - INFO ] Process 300-th batch and its stft mse: 2.724, sisnr: 0.189, lfb mse: 2.302, back loss is : 2.724 
2022-10-31 02:09:33 [265 - INFO ] Process 400-th batch and its stft mse: 3.026, sisnr: -0.022, lfb mse: 2.189, back loss is : 3.026 
2022-10-31 02:10:48 [265 - INFO ] Process 500-th batch and its stft mse: 3.226, sisnr: 0.070, lfb mse: 2.969, back loss is : 3.226 
2022-10-31 02:12:05 [265 - INFO ] Process 600-th batch and its stft mse: 2.561, sisnr: -0.115, lfb mse: 2.834, back loss is : 2.561 
2022-10-31 02:13:31 [265 - INFO ] Process 700-th batch and its stft mse: 2.950, sisnr: 0.759, lfb mse: 2.512, back loss is : 2.950 
2022-10-31 02:14:54 [265 - INFO ] Process 800-th batch and its stft mse: 2.656, sisnr: 0.306, lfb mse: 2.051, back loss is : 2.656 
2022-10-31 02:16:17 [265 - INFO ] Process 900-th batch and its stft mse: 2.796, sisnr: -0.123, lfb mse: 2.315, back loss is : 2.796 
2022-10-31 02:17:41 [265 - INFO ] Process 1000-th batch and its stft mse: 3.227, sisnr: 0.525, lfb mse: 2.963, back loss is : 3.227 
2022-10-31 02:19:01 [265 - INFO ] Process 1100-th batch and its stft mse: 3.362, sisnr: 0.607, lfb mse: 2.813, back loss is : 3.362 
2022-10-31 02:20:24 [265 - INFO ] Process 1200-th batch and its stft mse: 3.043, sisnr: 0.387, lfb mse: 2.508, back loss is : 3.043 
2022-10-31 02:21:47 [265 - INFO ] Process 1300-th batch and its stft mse: 2.591, sisnr: 0.018, lfb mse: 2.035, back loss is : 2.591 
2022-10-31 02:23:08 [265 - INFO ] Process 1400-th batch and its stft mse: 2.617, sisnr: 0.143, lfb mse: 2.129, back loss is : 2.617 
2022-10-31 02:24:30 [265 - INFO ] Process 1500-th batch and its stft mse: 3.112, sisnr: 0.174, lfb mse: 2.540, back loss is : 3.112 
2022-10-31 02:25:28 [331 - INFO ] Loss(time/N, lr=2.500e-04) - Epoch 25: train = +2.98732(1255.74s/1531) | dev_loss = +3.55847, (35.81s/4272), sisnr= 1.40894, lfb= 1.57446, stft= 3.55847 | no impr, best = 3.52913
2022-10-31 02:26:54 [265 - INFO ] Process 100-th batch and its stft mse: 2.881, sisnr: 0.866, lfb mse: 3.085, back loss is : 2.881 
2022-10-31 02:28:17 [265 - INFO ] Process 200-th batch and its stft mse: 3.289, sisnr: 0.222, lfb mse: 2.391, back loss is : 3.289 
2022-10-31 02:29:38 [265 - INFO ] Process 300-th batch and its stft mse: 2.707, sisnr: 1.142, lfb mse: 2.568, back loss is : 2.707 
2022-10-31 02:30:53 [265 - INFO ] Process 400-th batch and its stft mse: 3.589, sisnr: 1.393, lfb mse: 2.367, back loss is : 3.589 
2022-10-31 02:32:12 [265 - INFO ] Process 500-th batch and its stft mse: 2.478, sisnr: 0.151, lfb mse: 2.690, back loss is : 2.478 
2022-10-31 02:33:35 [265 - INFO ] Process 600-th batch and its stft mse: 2.836, sisnr: 0.992, lfb mse: 2.673, back loss is : 2.836 
2022-10-31 02:35:01 [265 - INFO ] Process 700-th batch and its stft mse: 3.309, sisnr: 0.052, lfb mse: 2.764, back loss is : 3.309 
2022-10-31 02:36:22 [265 - INFO ] Process 800-th batch and its stft mse: 2.766, sisnr: -0.490, lfb mse: 2.161, back loss is : 2.766 
2022-10-31 02:37:44 [265 - INFO ] Process 900-th batch and its stft mse: 3.007, sisnr: -0.316, lfb mse: 2.596, back loss is : 3.007 
2022-10-31 02:39:07 [265 - INFO ] Process 1000-th batch and its stft mse: 3.447, sisnr: 1.136, lfb mse: 2.838, back loss is : 3.447 
2022-10-31 02:40:29 [265 - INFO ] Process 1100-th batch and its stft mse: 2.762, sisnr: 0.928, lfb mse: 2.366, back loss is : 2.762 
2022-10-31 02:41:50 [265 - INFO ] Process 1200-th batch and its stft mse: 2.661, sisnr: -0.129, lfb mse: 2.099, back loss is : 2.661 
2022-10-31 02:43:10 [265 - INFO ] Process 1300-th batch and its stft mse: 2.933, sisnr: -0.299, lfb mse: 2.181, back loss is : 2.933 
2022-10-31 02:44:34 [265 - INFO ] Process 1400-th batch and its stft mse: 3.293, sisnr: 2.257, lfb mse: 2.727, back loss is : 3.293 
2022-10-31 02:45:55 [265 - INFO ] Process 1500-th batch and its stft mse: 2.738, sisnr: 0.919, lfb mse: 2.478, back loss is : 2.738 
2022-10-31 02:46:54 [331 - INFO ] Loss(time/N, lr=2.500e-04) - Epoch 26: train = +2.97262(1250.64s/1531) | dev_loss = +3.56770, (35.32s/4272), sisnr= 1.47470, lfb= 1.57991, stft= 3.56770 | no impr, best = 3.52913
2022-10-31 02:48:24 [265 - INFO ] Process 100-th batch and its stft mse: 2.795, sisnr: -0.495, lfb mse: 3.008, back loss is : 2.795 
2022-10-31 02:49:45 [265 - INFO ] Process 200-th batch and its stft mse: 2.300, sisnr: -0.455, lfb mse: 2.395, back loss is : 2.300 
2022-10-31 02:51:04 [265 - INFO ] Process 300-th batch and its stft mse: 2.670, sisnr: 0.019, lfb mse: 2.829, back loss is : 2.670 
2022-10-31 02:52:25 [265 - INFO ] Process 400-th batch and its stft mse: 3.220, sisnr: 1.210, lfb mse: 2.898, back loss is : 3.220 
2022-10-31 02:53:48 [265 - INFO ] Process 500-th batch and its stft mse: 2.710, sisnr: -0.574, lfb mse: 1.919, back loss is : 2.710 
2022-10-31 02:55:12 [265 - INFO ] Process 600-th batch and its stft mse: 3.257, sisnr: 2.208, lfb mse: 2.770, back loss is : 3.257 
2022-10-31 02:56:35 [265 - INFO ] Process 700-th batch and its stft mse: 3.157, sisnr: 0.808, lfb mse: 2.371, back loss is : 3.157 
2022-10-31 02:57:56 [265 - INFO ] Process 800-th batch and its stft mse: 2.953, sisnr: 0.594, lfb mse: 2.705, back loss is : 2.953 
2022-10-31 02:59:18 [265 - INFO ] Process 900-th batch and its stft mse: 2.852, sisnr: -0.172, lfb mse: 2.228, back loss is : 2.852 
2022-10-31 03:00:41 [265 - INFO ] Process 1000-th batch and its stft mse: 3.223, sisnr: 0.112, lfb mse: 3.165, back loss is : 3.223 
2022-10-31 03:02:03 [265 - INFO ] Process 1100-th batch and its stft mse: 3.136, sisnr: 0.028, lfb mse: 2.940, back loss is : 3.136 
2022-10-31 03:03:22 [265 - INFO ] Process 1200-th batch and its stft mse: 2.623, sisnr: 0.407, lfb mse: 2.456, back loss is : 2.623 
2022-10-31 03:04:43 [265 - INFO ] Process 1300-th batch and its stft mse: 3.135, sisnr: 0.037, lfb mse: 2.511, back loss is : 3.135 
2022-10-31 03:06:02 [265 - INFO ] Process 1400-th batch and its stft mse: 2.838, sisnr: -0.279, lfb mse: 1.736, back loss is : 2.838 
2022-10-31 03:07:25 [265 - INFO ] Process 1500-th batch and its stft mse: 3.143, sisnr: 0.517, lfb mse: 2.035, back loss is : 3.143 
2022-10-31 03:08:25 [331 - INFO ] Loss(time/N, lr=2.500e-04) - Epoch 27: train = +2.95903(1253.93s/1531) | dev_loss = +3.57169, (35.93s/4272), sisnr= 1.45995, lfb= 1.44926, stft= 3.57169 | no impr, best = 3.52913
2022-10-31 03:09:51 [265 - INFO ] Process 100-th batch and its stft mse: 2.636, sisnr: 0.128, lfb mse: 3.522, back loss is : 2.636 
2022-10-31 03:11:12 [265 - INFO ] Process 200-th batch and its stft mse: 3.326, sisnr: -0.051, lfb mse: 2.711, back loss is : 3.326 
2022-10-31 03:12:28 [265 - INFO ] Process 300-th batch and its stft mse: 2.990, sisnr: -0.434, lfb mse: 2.069, back loss is : 2.990 
2022-10-31 03:13:49 [265 - INFO ] Process 400-th batch and its stft mse: 2.806, sisnr: -0.606, lfb mse: 2.682, back loss is : 2.806 
2022-10-31 03:15:07 [265 - INFO ] Process 500-th batch and its stft mse: 3.138, sisnr: 0.037, lfb mse: 2.664, back loss is : 3.138 
2022-10-31 03:16:28 [265 - INFO ] Process 600-th batch and its stft mse: 2.959, sisnr: 0.170, lfb mse: 2.340, back loss is : 2.959 
2022-10-31 03:17:52 [265 - INFO ] Process 700-th batch and its stft mse: 2.980, sisnr: 0.547, lfb mse: 2.521, back loss is : 2.980 
2022-10-31 03:19:10 [265 - INFO ] Process 800-th batch and its stft mse: 3.297, sisnr: 0.916, lfb mse: 2.364, back loss is : 3.297 
2022-10-31 03:20:27 [265 - INFO ] Process 900-th batch and its stft mse: 3.104, sisnr: 0.299, lfb mse: 3.189, back loss is : 3.104 
2022-10-31 03:21:48 [265 - INFO ] Process 1000-th batch and its stft mse: 3.090, sisnr: 0.430, lfb mse: 2.783, back loss is : 3.090 
2022-10-31 03:23:10 [265 - INFO ] Process 1100-th batch and its stft mse: 2.961, sisnr: -0.746, lfb mse: 2.722, back loss is : 2.961 
2022-10-31 03:24:34 [265 - INFO ] Process 1200-th batch and its stft mse: 2.948, sisnr: 0.657, lfb mse: 2.903, back loss is : 2.948 
2022-10-31 03:25:55 [265 - INFO ] Process 1300-th batch and its stft mse: 3.128, sisnr: -0.146, lfb mse: 2.016, back loss is : 3.128 
2022-10-31 03:27:17 [265 - INFO ] Process 1400-th batch and its stft mse: 3.445, sisnr: 0.155, lfb mse: 2.569, back loss is : 3.445 
2022-10-31 03:28:40 [265 - INFO ] Process 1500-th batch and its stft mse: 3.108, sisnr: 0.685, lfb mse: 3.577, back loss is : 3.108 
2022-10-31 03:29:42 [331 - INFO ] Loss(time/N, lr=1.250e-04) - Epoch 28: train = +2.92180(1239.27s/1531) | dev_loss = +3.58509, (37.70s/4272), sisnr= 1.49834, lfb= 1.57228, stft= 3.58509 | no impr, best = 3.52913
2022-10-31 03:31:08 [265 - INFO ] Process 100-th batch and its stft mse: 2.853, sisnr: 0.291, lfb mse: 3.556, back loss is : 2.853 
2022-10-31 03:32:27 [265 - INFO ] Process 200-th batch and its stft mse: 2.804, sisnr: -0.121, lfb mse: 2.560, back loss is : 2.804 
2022-10-31 03:33:44 [265 - INFO ] Process 300-th batch and its stft mse: 2.916, sisnr: 0.256, lfb mse: 2.536, back loss is : 2.916 
2022-10-31 03:35:06 [265 - INFO ] Process 400-th batch and its stft mse: 3.339, sisnr: -0.235, lfb mse: 2.764, back loss is : 3.339 
2022-10-31 03:36:30 [265 - INFO ] Process 500-th batch and its stft mse: 3.146, sisnr: 0.439, lfb mse: 2.807, back loss is : 3.146 
2022-10-31 03:37:51 [265 - INFO ] Process 600-th batch and its stft mse: 2.687, sisnr: -0.344, lfb mse: 2.400, back loss is : 2.687 
2022-10-31 03:39:11 [265 - INFO ] Process 700-th batch and its stft mse: 3.052, sisnr: 0.657, lfb mse: 2.819, back loss is : 3.052 
2022-10-31 03:40:35 [265 - INFO ] Process 800-th batch and its stft mse: 3.160, sisnr: 0.586, lfb mse: 3.270, back loss is : 3.160 
2022-10-31 03:41:58 [265 - INFO ] Process 900-th batch and its stft mse: 2.797, sisnr: 0.311, lfb mse: 3.457, back loss is : 2.797 
2022-10-31 03:43:22 [265 - INFO ] Process 1000-th batch and its stft mse: 3.291, sisnr: -0.324, lfb mse: 2.894, back loss is : 3.291 
2022-10-31 03:44:41 [265 - INFO ] Process 1100-th batch and its stft mse: 2.831, sisnr: -0.071, lfb mse: 2.270, back loss is : 2.831 
2022-10-31 03:46:03 [265 - INFO ] Process 1200-th batch and its stft mse: 2.311, sisnr: 0.141, lfb mse: 2.629, back loss is : 2.311 
2022-10-31 03:47:24 [265 - INFO ] Process 1300-th batch and its stft mse: 3.038, sisnr: -0.989, lfb mse: 2.815, back loss is : 3.038 
2022-10-31 03:48:44 [265 - INFO ] Process 1400-th batch and its stft mse: 3.206, sisnr: 1.194, lfb mse: 3.226, back loss is : 3.206 
2022-10-31 03:50:03 [265 - INFO ] Process 1500-th batch and its stft mse: 2.308, sisnr: -0.933, lfb mse: 2.319, back loss is : 2.308 
2022-10-31 03:51:01 [331 - INFO ] Loss(time/N, lr=1.250e-04) - Epoch 29: train = +2.90329(1243.61s/1531) | dev_loss = +3.59067, (34.87s/4272), sisnr= 1.51506, lfb= 1.59415, stft= 3.59067 | no impr, best = 3.52913
2022-10-31 03:52:26 [265 - INFO ] Process 100-th batch and its stft mse: 2.752, sisnr: -0.320, lfb mse: 3.080, back loss is : 2.752 
2022-10-31 03:53:42 [265 - INFO ] Process 200-th batch and its stft mse: 2.852, sisnr: 0.209, lfb mse: 2.207, back loss is : 2.852 
2022-10-31 03:55:02 [265 - INFO ] Process 300-th batch and its stft mse: 2.813, sisnr: -0.594, lfb mse: 2.153, back loss is : 2.813 
2022-10-31 03:56:22 [265 - INFO ] Process 400-th batch and its stft mse: 2.679, sisnr: -0.678, lfb mse: 2.349, back loss is : 2.679 
2022-10-31 03:57:48 [265 - INFO ] Process 500-th batch and its stft mse: 2.686, sisnr: -0.170, lfb mse: 2.543, back loss is : 2.686 
2022-10-31 03:59:12 [265 - INFO ] Process 600-th batch and its stft mse: 3.012, sisnr: -0.403, lfb mse: 2.506, back loss is : 3.012 
2022-10-31 04:00:37 [265 - INFO ] Process 700-th batch and its stft mse: 2.600, sisnr: 0.005, lfb mse: 1.985, back loss is : 2.600 
2022-10-31 04:01:57 [265 - INFO ] Process 800-th batch and its stft mse: 2.852, sisnr: -0.807, lfb mse: 2.504, back loss is : 2.852 
2022-10-31 04:03:17 [265 - INFO ] Process 900-th batch and its stft mse: 2.532, sisnr: -0.667, lfb mse: 2.601, back loss is : 2.532 
2022-10-31 04:04:44 [265 - INFO ] Process 1000-th batch and its stft mse: 2.694, sisnr: 0.909, lfb mse: 2.816, back loss is : 2.694 
2022-10-31 04:06:04 [265 - INFO ] Process 1100-th batch and its stft mse: 3.078, sisnr: 0.267, lfb mse: 2.696, back loss is : 3.078 
2022-10-31 04:07:26 [265 - INFO ] Process 1200-th batch and its stft mse: 2.991, sisnr: 1.116, lfb mse: 3.027, back loss is : 2.991 
2022-10-31 04:08:49 [265 - INFO ] Process 1300-th batch and its stft mse: 3.011, sisnr: 0.302, lfb mse: 3.372, back loss is : 3.011 
2022-10-31 04:10:13 [265 - INFO ] Process 1400-th batch and its stft mse: 2.960, sisnr: 1.105, lfb mse: 2.884, back loss is : 2.960 
2022-10-31 04:11:33 [265 - INFO ] Process 1500-th batch and its stft mse: 3.163, sisnr: 0.337, lfb mse: 2.660, back loss is : 3.163 
2022-10-31 04:12:32 [331 - INFO ] Loss(time/N, lr=1.250e-04) - Epoch 30: train = +2.89300(1256.11s/1531) | dev_loss = +3.59398, (34.94s/4272), sisnr= 1.53693, lfb= 1.55336, stft= 3.59398 | no impr, best = 3.52913
2022-10-31 04:13:54 [265 - INFO ] Process 100-th batch and its stft mse: 2.440, sisnr: -1.035, lfb mse: 2.262, back loss is : 2.440 
2022-10-31 04:15:19 [265 - INFO ] Process 200-th batch and its stft mse: 2.599, sisnr: -0.225, lfb mse: 2.700, back loss is : 2.599 
2022-10-31 04:16:39 [265 - INFO ] Process 300-th batch and its stft mse: 3.119, sisnr: -0.401, lfb mse: 2.506, back loss is : 3.119 
2022-10-31 04:17:57 [265 - INFO ] Process 400-th batch and its stft mse: 2.907, sisnr: -0.277, lfb mse: 2.953, back loss is : 2.907 
2022-10-31 04:19:16 [265 - INFO ] Process 500-th batch and its stft mse: 2.955, sisnr: 0.517, lfb mse: 2.555, back loss is : 2.955 
2022-10-31 04:20:35 [265 - INFO ] Process 600-th batch and its stft mse: 3.015, sisnr: 0.177, lfb mse: 2.793, back loss is : 3.015 
2022-10-31 04:21:59 [265 - INFO ] Process 700-th batch and its stft mse: 3.255, sisnr: 0.557, lfb mse: 3.125, back loss is : 3.255 
2022-10-31 04:23:23 [265 - INFO ] Process 800-th batch and its stft mse: 2.981, sisnr: -0.040, lfb mse: 2.211, back loss is : 2.981 
2022-10-31 04:24:46 [265 - INFO ] Process 900-th batch and its stft mse: 3.122, sisnr: 0.445, lfb mse: 2.385, back loss is : 3.122 
2022-10-31 04:26:12 [265 - INFO ] Process 1000-th batch and its stft mse: 3.051, sisnr: -0.245, lfb mse: 2.184, back loss is : 3.051 
2022-10-31 04:27:41 [265 - INFO ] Process 1100-th batch and its stft mse: 2.763, sisnr: 0.019, lfb mse: 2.067, back loss is : 2.763 
2022-10-31 04:29:02 [265 - INFO ] Process 1200-th batch and its stft mse: 2.899, sisnr: -0.218, lfb mse: 2.823, back loss is : 2.899 
2022-10-31 04:30:22 [265 - INFO ] Process 1300-th batch and its stft mse: 2.206, sisnr: -1.073, lfb mse: 2.518, back loss is : 2.206 
2022-10-31 04:31:43 [265 - INFO ] Process 1400-th batch and its stft mse: 2.837, sisnr: 0.438, lfb mse: 3.339, back loss is : 2.837 
2022-10-31 04:33:05 [265 - INFO ] Process 1500-th batch and its stft mse: 3.306, sisnr: 0.068, lfb mse: 2.676, back loss is : 3.306 
2022-10-31 04:34:04 [331 - INFO ] Loss(time/N, lr=1.250e-04) - Epoch 31: train = +2.88511(1257.72s/1531) | dev_loss = +3.61296, (34.43s/4272), sisnr= 1.57971, lfb= 1.57845, stft= 3.61296 | no impr, best = 3.52913
2022-10-31 04:35:28 [265 - INFO ] Process 100-th batch and its stft mse: 2.501, sisnr: -1.074, lfb mse: 2.097, back loss is : 2.501 
2022-10-31 04:36:48 [265 - INFO ] Process 200-th batch and its stft mse: 2.380, sisnr: -0.816, lfb mse: 2.387, back loss is : 2.380 
2022-10-31 04:38:10 [265 - INFO ] Process 300-th batch and its stft mse: 3.280, sisnr: 0.324, lfb mse: 2.921, back loss is : 3.280 
2022-10-31 04:39:31 [265 - INFO ] Process 400-th batch and its stft mse: 2.972, sisnr: 0.449, lfb mse: 2.476, back loss is : 2.972 
2022-10-31 04:40:54 [265 - INFO ] Process 500-th batch and its stft mse: 3.099, sisnr: 0.272, lfb mse: 2.522, back loss is : 3.099 
2022-10-31 04:42:18 [265 - INFO ] Process 600-th batch and its stft mse: 2.681, sisnr: -0.154, lfb mse: 2.968, back loss is : 2.681 
2022-10-31 04:43:40 [265 - INFO ] Process 700-th batch and its stft mse: 2.811, sisnr: 0.620, lfb mse: 2.580, back loss is : 2.811 
2022-10-31 04:45:02 [265 - INFO ] Process 800-th batch and its stft mse: 3.253, sisnr: 0.962, lfb mse: 2.512, back loss is : 3.253 
2022-10-31 04:46:22 [265 - INFO ] Process 900-th batch and its stft mse: 2.986, sisnr: -0.388, lfb mse: 2.552, back loss is : 2.986 
2022-10-31 04:47:43 [265 - INFO ] Process 1000-th batch and its stft mse: 3.010, sisnr: 0.035, lfb mse: 2.319, back loss is : 3.010 
2022-10-31 04:49:01 [265 - INFO ] Process 1100-th batch and its stft mse: 3.079, sisnr: 0.348, lfb mse: 2.980, back loss is : 3.079 
2022-10-31 04:50:22 [265 - INFO ] Process 1200-th batch and its stft mse: 2.857, sisnr: 0.089, lfb mse: 3.309, back loss is : 2.857 
2022-10-31 04:51:45 [265 - INFO ] Process 1300-th batch and its stft mse: 3.292, sisnr: -0.084, lfb mse: 2.905, back loss is : 3.292 
2022-10-31 04:53:09 [265 - INFO ] Process 1400-th batch and its stft mse: 2.714, sisnr: -0.681, lfb mse: 2.472, back loss is : 2.714 
2022-10-31 04:54:34 [265 - INFO ] Process 1500-th batch and its stft mse: 2.863, sisnr: 0.433, lfb mse: 3.290, back loss is : 2.863 
2022-10-31 04:55:31 [331 - INFO ] Loss(time/N, lr=6.250e-05) - Epoch 32: train = +2.86262(1250.82s/1531) | dev_loss = +3.61639, (35.79s/4272), sisnr= 1.60914, lfb= 1.56618, stft= 3.61639 | no impr, best = 3.52913
2022-10-31 04:57:01 [265 - INFO ] Process 100-th batch and its stft mse: 2.784, sisnr: -0.038, lfb mse: 2.136, back loss is : 2.784 
2022-10-31 04:58:24 [265 - INFO ] Process 200-th batch and its stft mse: 3.084, sisnr: 0.884, lfb mse: 3.579, back loss is : 3.084 
2022-10-31 04:59:43 [265 - INFO ] Process 300-th batch and its stft mse: 2.860, sisnr: 0.146, lfb mse: 2.397, back loss is : 2.860 
2022-10-31 05:01:08 [265 - INFO ] Process 400-th batch and its stft mse: 2.924, sisnr: -0.032, lfb mse: 2.807, back loss is : 2.924 
2022-10-31 05:02:30 [265 - INFO ] Process 500-th batch and its stft mse: 3.036, sisnr: 0.074, lfb mse: 2.561, back loss is : 3.036 
2022-10-31 05:03:51 [265 - INFO ] Process 600-th batch and its stft mse: 2.596, sisnr: -0.416, lfb mse: 2.897, back loss is : 2.596 
2022-10-31 05:05:15 [265 - INFO ] Process 700-th batch and its stft mse: 3.465, sisnr: -0.015, lfb mse: 2.293, back loss is : 3.465 
2022-10-31 05:06:38 [265 - INFO ] Process 800-th batch and its stft mse: 3.044, sisnr: 1.012, lfb mse: 3.280, back loss is : 3.044 
2022-10-31 05:07:58 [265 - INFO ] Process 900-th batch and its stft mse: 2.700, sisnr: -1.367, lfb mse: 2.227, back loss is : 2.700 
2022-10-31 05:09:14 [265 - INFO ] Process 1000-th batch and its stft mse: 2.839, sisnr: -0.185, lfb mse: 2.650, back loss is : 2.839 
2022-10-31 05:10:33 [265 - INFO ] Process 1100-th batch and its stft mse: 3.271, sisnr: 0.545, lfb mse: 2.790, back loss is : 3.271 
2022-10-31 05:11:53 [265 - INFO ] Process 1200-th batch and its stft mse: 2.752, sisnr: 0.022, lfb mse: 2.367, back loss is : 2.752 
2022-10-31 05:13:14 [265 - INFO ] Process 1300-th batch and its stft mse: 3.010, sisnr: -0.333, lfb mse: 2.403, back loss is : 3.010 
2022-10-31 05:14:37 [265 - INFO ] Process 1400-th batch and its stft mse: 2.780, sisnr: 0.318, lfb mse: 2.582, back loss is : 2.780 
2022-10-31 05:15:55 [265 - INFO ] Process 1500-th batch and its stft mse: 3.299, sisnr: -0.271, lfb mse: 2.355, back loss is : 3.299 
2022-10-31 05:16:53 [331 - INFO ] Loss(time/N, lr=6.250e-05) - Epoch 33: train = +2.85228(1247.43s/1531) | dev_loss = +3.61922, (34.15s/4272), sisnr= 1.61006, lfb= 1.57108, stft= 3.61922 | no impr, best = 3.52913
2022-10-31 05:18:19 [265 - INFO ] Process 100-th batch and its stft mse: 3.770, sisnr: 0.021, lfb mse: 1.929, back loss is : 3.770 
2022-10-31 05:19:43 [265 - INFO ] Process 200-th batch and its stft mse: 2.831, sisnr: 0.161, lfb mse: 2.260, back loss is : 2.831 
2022-10-31 05:21:05 [265 - INFO ] Process 300-th batch and its stft mse: 2.791, sisnr: -0.592, lfb mse: 2.736, back loss is : 2.791 
2022-10-31 05:22:27 [265 - INFO ] Process 400-th batch and its stft mse: 3.039, sisnr: 0.297, lfb mse: 3.060, back loss is : 3.039 
2022-10-31 05:23:47 [265 - INFO ] Process 500-th batch and its stft mse: 3.381, sisnr: 0.087, lfb mse: 3.048, back loss is : 3.381 
2022-10-31 05:25:07 [265 - INFO ] Process 600-th batch and its stft mse: 2.631, sisnr: -0.602, lfb mse: 2.809, back loss is : 2.631 
2022-10-31 05:26:26 [265 - INFO ] Process 700-th batch and its stft mse: 2.900, sisnr: -0.590, lfb mse: 2.746, back loss is : 2.900 
2022-10-31 05:27:47 [265 - INFO ] Process 800-th batch and its stft mse: 3.174, sisnr: 0.406, lfb mse: 3.094, back loss is : 3.174 
2022-10-31 05:29:08 [265 - INFO ] Process 900-th batch and its stft mse: 2.907, sisnr: -0.062, lfb mse: 3.001, back loss is : 2.907 
2022-10-31 05:30:33 [265 - INFO ] Process 1000-th batch and its stft mse: 2.716, sisnr: -1.164, lfb mse: 2.567, back loss is : 2.716 
2022-10-31 05:31:53 [265 - INFO ] Process 1100-th batch and its stft mse: 3.248, sisnr: -0.195, lfb mse: 2.631, back loss is : 3.248 
2022-10-31 05:33:14 [265 - INFO ] Process 1200-th batch and its stft mse: 3.108, sisnr: -0.072, lfb mse: 2.970, back loss is : 3.108 
2022-10-31 05:34:36 [265 - INFO ] Process 1300-th batch and its stft mse: 2.745, sisnr: 0.198, lfb mse: 3.075, back loss is : 2.745 
2022-10-31 05:35:57 [265 - INFO ] Process 1400-th batch and its stft mse: 2.676, sisnr: -0.580, lfb mse: 2.637, back loss is : 2.676 
2022-10-31 05:37:15 [265 - INFO ] Process 1500-th batch and its stft mse: 2.961, sisnr: -0.189, lfb mse: 3.081, back loss is : 2.961 
2022-10-31 05:38:15 [331 - INFO ] Loss(time/N, lr=6.250e-05) - Epoch 34: train = +2.84769(1246.34s/1531) | dev_loss = +3.62221, (35.23s/4272), sisnr= 1.61218, lfb= 1.58437, stft= 3.62221 | no impr, best = 3.52913
2022-10-31 05:39:40 [265 - INFO ] Process 100-th batch and its stft mse: 2.695, sisnr: -0.801, lfb mse: 2.391, back loss is : 2.695 
2022-10-31 05:41:04 [265 - INFO ] Process 200-th batch and its stft mse: 3.112, sisnr: -0.549, lfb mse: 3.262, back loss is : 3.112 
2022-10-31 05:42:21 [265 - INFO ] Process 300-th batch and its stft mse: 2.592, sisnr: -0.762, lfb mse: 2.236, back loss is : 2.592 
2022-10-31 05:43:45 [265 - INFO ] Process 400-th batch and its stft mse: 2.364, sisnr: -0.515, lfb mse: 2.534, back loss is : 2.364 
2022-10-31 05:45:03 [265 - INFO ] Process 500-th batch and its stft mse: 2.724, sisnr: -0.290, lfb mse: 2.534, back loss is : 2.724 
2022-10-31 05:46:27 [265 - INFO ] Process 600-th batch and its stft mse: 2.489, sisnr: -0.413, lfb mse: 2.662, back loss is : 2.489 
2022-10-31 05:47:50 [265 - INFO ] Process 700-th batch and its stft mse: 3.300, sisnr: -0.549, lfb mse: 2.403, back loss is : 3.300 
2022-10-31 05:49:13 [265 - INFO ] Process 800-th batch and its stft mse: 2.185, sisnr: -1.288, lfb mse: 2.251, back loss is : 2.185 
2022-10-31 05:50:34 [265 - INFO ] Process 900-th batch and its stft mse: 2.681, sisnr: -0.710, lfb mse: 2.527, back loss is : 2.681 
2022-10-31 05:51:58 [265 - INFO ] Process 1000-th batch and its stft mse: 2.915, sisnr: -0.551, lfb mse: 2.911, back loss is : 2.915 
2022-10-31 05:53:19 [265 - INFO ] Process 1100-th batch and its stft mse: 2.884, sisnr: -0.958, lfb mse: 2.569, back loss is : 2.884 
2022-10-31 05:54:38 [265 - INFO ] Process 1200-th batch and its stft mse: 2.886, sisnr: -0.342, lfb mse: 2.064, back loss is : 2.886 
2022-10-31 05:56:01 [265 - INFO ] Process 1300-th batch and its stft mse: 2.616, sisnr: -0.302, lfb mse: 2.196, back loss is : 2.616 
2022-10-31 05:57:18 [265 - INFO ] Process 1400-th batch and its stft mse: 2.843, sisnr: -0.409, lfb mse: 3.145, back loss is : 2.843 
2022-10-31 05:58:41 [265 - INFO ] Process 1500-th batch and its stft mse: 3.010, sisnr: -0.033, lfb mse: 2.571, back loss is : 3.010 
2022-10-31 05:59:44 [331 - INFO ] Loss(time/N, lr=6.250e-05) - Epoch 35: train = +2.84205(1251.18s/1531) | dev_loss = +3.62839, (37.50s/4272), sisnr= 1.62772, lfb= 1.58197, stft= 3.62839 | no impr, best = 3.52913
2022-10-31 06:01:07 [265 - INFO ] Process 100-th batch and its stft mse: 2.742, sisnr: -0.465, lfb mse: 2.535, back loss is : 2.742 
2022-10-31 06:02:25 [265 - INFO ] Process 200-th batch and its stft mse: 2.994, sisnr: 0.224, lfb mse: 2.804, back loss is : 2.994 
2022-10-31 06:03:46 [265 - INFO ] Process 300-th batch and its stft mse: 3.122, sisnr: 0.222, lfb mse: 3.125, back loss is : 3.122 
2022-10-31 06:05:06 [265 - INFO ] Process 400-th batch and its stft mse: 2.911, sisnr: -0.198, lfb mse: 2.786, back loss is : 2.911 
2022-10-31 06:06:23 [265 - INFO ] Process 500-th batch and its stft mse: 3.111, sisnr: -0.428, lfb mse: 2.173, back loss is : 3.111 
2022-10-31 06:07:45 [265 - INFO ] Process 600-th batch and its stft mse: 3.118, sisnr: 0.294, lfb mse: 2.684, back loss is : 3.118 
2022-10-31 06:09:04 [265 - INFO ] Process 700-th batch and its stft mse: 3.003, sisnr: -0.854, lfb mse: 2.395, back loss is : 3.003 
2022-10-31 06:10:24 [265 - INFO ] Process 800-th batch and its stft mse: 2.995, sisnr: -0.212, lfb mse: 2.423, back loss is : 2.995 
2022-10-31 06:11:47 [265 - INFO ] Process 900-th batch and its stft mse: 3.054, sisnr: 0.380, lfb mse: 2.273, back loss is : 3.054 
2022-10-31 06:13:08 [265 - INFO ] Process 1000-th batch and its stft mse: 2.695, sisnr: -0.063, lfb mse: 3.258, back loss is : 2.695 
2022-10-31 06:14:27 [265 - INFO ] Process 1100-th batch and its stft mse: 2.572, sisnr: -0.249, lfb mse: 2.239, back loss is : 2.572 
2022-10-31 06:15:49 [265 - INFO ] Process 1200-th batch and its stft mse: 2.811, sisnr: -0.592, lfb mse: 2.245, back loss is : 2.811 
2022-10-31 06:17:07 [265 - INFO ] Process 1300-th batch and its stft mse: 2.898, sisnr: -0.465, lfb mse: 2.694, back loss is : 2.898 
2022-10-31 06:18:25 [265 - INFO ] Process 1400-th batch and its stft mse: 2.782, sisnr: 0.460, lfb mse: 2.370, back loss is : 2.782 
2022-10-31 06:19:44 [265 - INFO ] Process 1500-th batch and its stft mse: 2.357, sisnr: 0.106, lfb mse: 2.475, back loss is : 2.357 
2022-10-31 06:20:43 [331 - INFO ] Loss(time/N, lr=3.125e-05) - Epoch 36: train = +2.82964(1223.58s/1531) | dev_loss = +3.63015, (35.49s/4272), sisnr= 1.62919, lfb= 1.54175, stft= 3.63015 | no impr, best = 3.52913
2022-10-31 06:22:11 [265 - INFO ] Process 100-th batch and its stft mse: 2.475, sisnr: -0.512, lfb mse: 2.167, back loss is : 2.475 
2022-10-31 06:23:32 [265 - INFO ] Process 200-th batch and its stft mse: 2.790, sisnr: -0.820, lfb mse: 2.424, back loss is : 2.790 
2022-10-31 06:24:50 [265 - INFO ] Process 300-th batch and its stft mse: 2.810, sisnr: -0.806, lfb mse: 2.480, back loss is : 2.810 
2022-10-31 06:26:13 [265 - INFO ] Process 400-th batch and its stft mse: 2.916, sisnr: 0.617, lfb mse: 2.810, back loss is : 2.916 
2022-10-31 06:27:40 [265 - INFO ] Process 500-th batch and its stft mse: 2.954, sisnr: 0.078, lfb mse: 2.608, back loss is : 2.954 
2022-10-31 06:29:04 [265 - INFO ] Process 600-th batch and its stft mse: 3.077, sisnr: -0.544, lfb mse: 1.943, back loss is : 3.077 
2022-10-31 06:30:27 [265 - INFO ] Process 700-th batch and its stft mse: 2.786, sisnr: -0.688, lfb mse: 2.549, back loss is : 2.786 
2022-10-31 06:31:49 [265 - INFO ] Process 800-th batch and its stft mse: 2.414, sisnr: -1.079, lfb mse: 2.555, back loss is : 2.414 
2022-10-31 06:33:09 [265 - INFO ] Process 900-th batch and its stft mse: 3.067, sisnr: 0.244, lfb mse: 2.718, back loss is : 3.067 
2022-10-31 06:34:33 [265 - INFO ] Process 1000-th batch and its stft mse: 2.198, sisnr: -1.163, lfb mse: 2.605, back loss is : 2.198 
2022-10-31 06:35:52 [265 - INFO ] Process 1100-th batch and its stft mse: 2.954, sisnr: -0.422, lfb mse: 2.195, back loss is : 2.954 
2022-10-31 06:37:12 [265 - INFO ] Process 1200-th batch and its stft mse: 2.554, sisnr: -0.176, lfb mse: 2.816, back loss is : 2.554 
2022-10-31 06:38:28 [265 - INFO ] Process 1300-th batch and its stft mse: 2.461, sisnr: -0.277, lfb mse: 2.389, back loss is : 2.461 
2022-10-31 06:39:50 [265 - INFO ] Process 1400-th batch and its stft mse: 2.885, sisnr: -0.458, lfb mse: 2.179, back loss is : 2.885 
2022-10-31 06:41:07 [265 - INFO ] Process 1500-th batch and its stft mse: 3.011, sisnr: -0.366, lfb mse: 2.116, back loss is : 3.011 
2022-10-31 06:42:05 [331 - INFO ] Loss(time/N, lr=3.125e-05) - Epoch 37: train = +2.82525(1246.99s/1531) | dev_loss = +3.63845, (34.86s/4272), sisnr= 1.66344, lfb= 1.62023, stft= 3.63845 | no impr, best = 3.52913
2022-10-31 06:43:30 [265 - INFO ] Process 100-th batch and its stft mse: 2.294, sisnr: -0.438, lfb mse: 2.441, back loss is : 2.294 
2022-10-31 06:44:51 [265 - INFO ] Process 200-th batch and its stft mse: 2.376, sisnr: -1.379, lfb mse: 2.039, back loss is : 2.376 
2022-10-31 06:46:17 [265 - INFO ] Process 300-th batch and its stft mse: 3.225, sisnr: 0.107, lfb mse: 2.249, back loss is : 3.225 
2022-10-31 06:47:41 [265 - INFO ] Process 400-th batch and its stft mse: 2.852, sisnr: 0.321, lfb mse: 2.964, back loss is : 2.852 
2022-10-31 06:49:00 [265 - INFO ] Process 500-th batch and its stft mse: 2.989, sisnr: 0.272, lfb mse: 2.252, back loss is : 2.989 
2022-10-31 06:50:20 [265 - INFO ] Process 600-th batch and its stft mse: 2.805, sisnr: 0.212, lfb mse: 3.098, back loss is : 2.805 
2022-10-31 06:51:42 [265 - INFO ] Process 700-th batch and its stft mse: 2.946, sisnr: 0.117, lfb mse: 2.789, back loss is : 2.946 
2022-10-31 06:53:02 [265 - INFO ] Process 800-th batch and its stft mse: 3.067, sisnr: -0.045, lfb mse: 2.621, back loss is : 3.067 
2022-10-31 06:54:25 [265 - INFO ] Process 900-th batch and its stft mse: 2.374, sisnr: -1.239, lfb mse: 2.332, back loss is : 2.374 
2022-10-31 06:55:47 [265 - INFO ] Process 1000-th batch and its stft mse: 2.522, sisnr: -0.885, lfb mse: 2.020, back loss is : 2.522 
2022-10-31 06:57:07 [265 - INFO ] Process 1100-th batch and its stft mse: 2.658, sisnr: -0.211, lfb mse: 2.471, back loss is : 2.658 
2022-10-31 06:58:26 [265 - INFO ] Process 1200-th batch and its stft mse: 2.821, sisnr: -0.136, lfb mse: 2.157, back loss is : 2.821 
2022-10-31 06:59:45 [265 - INFO ] Process 1300-th batch and its stft mse: 2.642, sisnr: -0.066, lfb mse: 2.378, back loss is : 2.642 
2022-10-31 07:01:07 [265 - INFO ] Process 1400-th batch and its stft mse: 2.897, sisnr: 0.609, lfb mse: 2.680, back loss is : 2.897 
2022-10-31 07:02:26 [265 - INFO ] Process 1500-th batch and its stft mse: 2.767, sisnr: -0.784, lfb mse: 2.345, back loss is : 2.767 
2022-10-31 07:03:27 [331 - INFO ] Loss(time/N, lr=3.125e-05) - Epoch 38: train = +2.82231(1245.53s/1531) | dev_loss = +3.63810, (35.76s/4272), sisnr= 1.63907, lfb= 1.50913, stft= 3.63810 | no impr, best = 3.52913
2022-10-31 07:04:51 [265 - INFO ] Process 100-th batch and its stft mse: 3.374, sisnr: 0.109, lfb mse: 2.813, back loss is : 3.374 
2022-10-31 07:06:14 [265 - INFO ] Process 200-th batch and its stft mse: 3.221, sisnr: 0.570, lfb mse: 2.267, back loss is : 3.221 
2022-10-31 07:07:33 [265 - INFO ] Process 300-th batch and its stft mse: 2.861, sisnr: -0.837, lfb mse: 2.222, back loss is : 2.861 
2022-10-31 07:08:58 [265 - INFO ] Process 400-th batch and its stft mse: 2.992, sisnr: -0.167, lfb mse: 2.852, back loss is : 2.992 
2022-10-31 07:10:17 [265 - INFO ] Process 500-th batch and its stft mse: 3.067, sisnr: 0.349, lfb mse: 3.241, back loss is : 3.067 
2022-10-31 07:11:39 [265 - INFO ] Process 600-th batch and its stft mse: 2.900, sisnr: 0.552, lfb mse: 3.518, back loss is : 2.900 
2022-10-31 07:12:59 [265 - INFO ] Process 700-th batch and its stft mse: 2.833, sisnr: -0.099, lfb mse: 2.350, back loss is : 2.833 
2022-10-31 07:14:18 [265 - INFO ] Process 800-th batch and its stft mse: 2.869, sisnr: 0.002, lfb mse: 2.369, back loss is : 2.869 
2022-10-31 07:15:40 [265 - INFO ] Process 900-th batch and its stft mse: 2.995, sisnr: 0.273, lfb mse: 2.983, back loss is : 2.995 
2022-10-31 07:17:03 [265 - INFO ] Process 1000-th batch and its stft mse: 2.880, sisnr: -0.839, lfb mse: 2.708, back loss is : 2.880 
2022-10-31 07:18:22 [265 - INFO ] Process 1100-th batch and its stft mse: 2.723, sisnr: -0.620, lfb mse: 2.065, back loss is : 2.723 
2022-10-31 07:19:39 [265 - INFO ] Process 1200-th batch and its stft mse: 2.590, sisnr: -0.963, lfb mse: 2.256, back loss is : 2.590 
2022-10-31 07:21:01 [265 - INFO ] Process 1300-th batch and its stft mse: 2.933, sisnr: -0.063, lfb mse: 2.637, back loss is : 2.933 
2022-10-31 07:22:26 [265 - INFO ] Process 1400-th batch and its stft mse: 2.856, sisnr: -0.891, lfb mse: 2.382, back loss is : 2.856 
2022-10-31 07:23:49 [265 - INFO ] Process 1500-th batch and its stft mse: 2.851, sisnr: -0.259, lfb mse: 2.580, back loss is : 2.851 
2022-10-31 07:24:48 [331 - INFO ] Loss(time/N, lr=3.125e-05) - Epoch 39: train = +2.82049(1245.23s/1531) | dev_loss = +3.64668, (36.02s/4272), sisnr= 1.67268, lfb= 1.54097, stft= 3.64668 | no impr, best = 3.52913
2022-10-31 07:26:12 [265 - INFO ] Process 100-th batch and its stft mse: 2.326, sisnr: -0.986, lfb mse: 2.103, back loss is : 2.326 
2022-10-31 07:27:35 [265 - INFO ] Process 200-th batch and its stft mse: 2.618, sisnr: -0.712, lfb mse: 2.590, back loss is : 2.618 
2022-10-31 07:28:59 [265 - INFO ] Process 300-th batch and its stft mse: 2.939, sisnr: 0.284, lfb mse: 2.509, back loss is : 2.939 
2022-10-31 07:30:20 [265 - INFO ] Process 400-th batch and its stft mse: 3.305, sisnr: 0.102, lfb mse: 2.935, back loss is : 3.305 
2022-10-31 07:31:47 [265 - INFO ] Process 500-th batch and its stft mse: 2.568, sisnr: -0.977, lfb mse: 2.515, back loss is : 2.568 
2022-10-31 07:33:10 [265 - INFO ] Process 600-th batch and its stft mse: 3.028, sisnr: 0.104, lfb mse: 2.794, back loss is : 3.028 
2022-10-31 07:34:34 [265 - INFO ] Process 700-th batch and its stft mse: 2.824, sisnr: -0.552, lfb mse: 3.008, back loss is : 2.824 
2022-10-31 07:36:02 [265 - INFO ] Process 800-th batch and its stft mse: 2.734, sisnr: -0.519, lfb mse: 2.294, back loss is : 2.734 
2022-10-31 07:37:24 [265 - INFO ] Process 900-th batch and its stft mse: 2.728, sisnr: -1.240, lfb mse: 2.493, back loss is : 2.728 
2022-10-31 07:38:45 [265 - INFO ] Process 1000-th batch and its stft mse: 2.433, sisnr: -0.400, lfb mse: 2.633, back loss is : 2.433 
2022-10-31 07:40:02 [265 - INFO ] Process 1100-th batch and its stft mse: 2.746, sisnr: -0.242, lfb mse: 2.634, back loss is : 2.746 
2022-10-31 07:41:23 [265 - INFO ] Process 1200-th batch and its stft mse: 2.770, sisnr: -0.425, lfb mse: 3.393, back loss is : 2.770 
2022-10-31 07:42:44 [265 - INFO ] Process 1300-th batch and its stft mse: 2.381, sisnr: -0.857, lfb mse: 2.261, back loss is : 2.381 
2022-10-31 07:44:04 [265 - INFO ] Process 1400-th batch and its stft mse: 2.487, sisnr: -1.001, lfb mse: 2.381, back loss is : 2.487 
2022-10-31 07:45:29 [265 - INFO ] Process 1500-th batch and its stft mse: 2.538, sisnr: -1.347, lfb mse: 2.239, back loss is : 2.538 
2022-10-31 07:46:33 [331 - INFO ] Loss(time/N, lr=1.563e-05) - Epoch 40: train = +2.81190(1266.47s/1531) | dev_loss = +3.64703, (37.90s/4272), sisnr= 1.67210, lfb= 1.58033, stft= 3.64703 | no impr, best = 3.52913
2022-10-31 07:47:58 [265 - INFO ] Process 100-th batch and its stft mse: 2.725, sisnr: -0.739, lfb mse: 2.552, back loss is : 2.725 
2022-10-31 07:49:17 [265 - INFO ] Process 200-th batch and its stft mse: 2.897, sisnr: 0.414, lfb mse: 2.477, back loss is : 2.897 
2022-10-31 07:50:37 [265 - INFO ] Process 300-th batch and its stft mse: 2.814, sisnr: -0.288, lfb mse: 3.267, back loss is : 2.814 
2022-10-31 07:51:58 [265 - INFO ] Process 400-th batch and its stft mse: 2.583, sisnr: -0.662, lfb mse: 2.896, back loss is : 2.583 
2022-10-31 07:53:16 [265 - INFO ] Process 500-th batch and its stft mse: 2.908, sisnr: -0.418, lfb mse: 2.905, back loss is : 2.908 
2022-10-31 07:54:36 [265 - INFO ] Process 600-th batch and its stft mse: 2.723, sisnr: -1.622, lfb mse: 2.808, back loss is : 2.723 
2022-10-31 07:55:58 [265 - INFO ] Process 700-th batch and its stft mse: 2.697, sisnr: -0.303, lfb mse: 2.522, back loss is : 2.697 
2022-10-31 07:57:22 [265 - INFO ] Process 800-th batch and its stft mse: 3.319, sisnr: -0.301, lfb mse: 2.453, back loss is : 3.319 
2022-10-31 07:58:42 [265 - INFO ] Process 900-th batch and its stft mse: 3.298, sisnr: -0.416, lfb mse: 2.785, back loss is : 3.298 
2022-10-31 07:59:59 [265 - INFO ] Process 1000-th batch and its stft mse: 2.898, sisnr: -1.138, lfb mse: 2.300, back loss is : 2.898 
2022-10-31 08:01:21 [265 - INFO ] Process 1100-th batch and its stft mse: 2.923, sisnr: -0.515, lfb mse: 2.177, back loss is : 2.923 
2022-10-31 08:02:42 [265 - INFO ] Process 1200-th batch and its stft mse: 2.680, sisnr: -0.767, lfb mse: 2.691, back loss is : 2.680 
2022-10-31 08:04:03 [265 - INFO ] Process 1300-th batch and its stft mse: 2.819, sisnr: 0.153, lfb mse: 2.501, back loss is : 2.819 
2022-10-31 08:05:23 [265 - INFO ] Process 1400-th batch and its stft mse: 2.725, sisnr: -0.419, lfb mse: 2.324, back loss is : 2.725 
2022-10-31 08:06:43 [265 - INFO ] Process 1500-th batch and its stft mse: 2.697, sisnr: -0.930, lfb mse: 2.649, back loss is : 2.697 
2022-10-31 08:07:41 [331 - INFO ] Loss(time/N, lr=1.563e-05) - Epoch 41: train = +2.81138(1233.19s/1531) | dev_loss = +3.65081, (34.82s/4272), sisnr= 1.69524, lfb= 1.58842, stft= 3.65081 | no impr, best = 3.52913
2022-10-31 08:09:08 [265 - INFO ] Process 100-th batch and its stft mse: 3.196, sisnr: -1.114, lfb mse: 2.392, back loss is : 3.196 
2022-10-31 08:10:30 [265 - INFO ] Process 200-th batch and its stft mse: 2.766, sisnr: -0.919, lfb mse: 2.404, back loss is : 2.766 
2022-10-31 08:11:52 [265 - INFO ] Process 300-th batch and its stft mse: 2.918, sisnr: -0.744, lfb mse: 2.661, back loss is : 2.918 
2022-10-31 08:13:19 [265 - INFO ] Process 400-th batch and its stft mse: 3.293, sisnr: -0.027, lfb mse: 3.065, back loss is : 3.293 
2022-10-31 08:14:43 [265 - INFO ] Process 500-th batch and its stft mse: 3.219, sisnr: 0.449, lfb mse: 2.411, back loss is : 3.219 
2022-10-31 08:16:09 [265 - INFO ] Process 600-th batch and its stft mse: 2.515, sisnr: 0.045, lfb mse: 2.316, back loss is : 2.515 
2022-10-31 08:17:33 [265 - INFO ] Process 700-th batch and its stft mse: 2.677, sisnr: 0.251, lfb mse: 3.027, back loss is : 2.677 
2022-10-31 08:18:54 [265 - INFO ] Process 800-th batch and its stft mse: 2.961, sisnr: 0.152, lfb mse: 2.867, back loss is : 2.961 
2022-10-31 08:20:15 [265 - INFO ] Process 900-th batch and its stft mse: 3.441, sisnr: 0.417, lfb mse: 2.336, back loss is : 3.441 
2022-10-31 08:21:30 [265 - INFO ] Process 1000-th batch and its stft mse: 2.676, sisnr: -0.402, lfb mse: 2.725, back loss is : 2.676 
2022-10-31 08:22:51 [265 - INFO ] Process 1100-th batch and its stft mse: 2.717, sisnr: -0.627, lfb mse: 2.569, back loss is : 2.717 
2022-10-31 08:24:13 [265 - INFO ] Process 1200-th batch and its stft mse: 2.877, sisnr: -0.780, lfb mse: 2.412, back loss is : 2.877 
2022-10-31 08:25:38 [265 - INFO ] Process 1300-th batch and its stft mse: 2.660, sisnr: 0.691, lfb mse: 2.549, back loss is : 2.660 
2022-10-31 08:27:00 [265 - INFO ] Process 1400-th batch and its stft mse: 2.415, sisnr: -1.009, lfb mse: 2.529, back loss is : 2.415 
2022-10-31 08:28:20 [265 - INFO ] Process 1500-th batch and its stft mse: 2.442, sisnr: -0.714, lfb mse: 1.868, back loss is : 2.442 
2022-10-31 08:29:19 [331 - INFO ] Loss(time/N, lr=1.563e-05) - Epoch 42: train = +2.80939(1260.23s/1531) | dev_loss = +3.65025, (37.21s/4272), sisnr= 1.66781, lfb= 1.51173, stft= 3.65025 | no impr, best = 3.52913
2022-10-31 08:30:46 [265 - INFO ] Process 100-th batch and its stft mse: 2.836, sisnr: -0.572, lfb mse: 3.205, back loss is : 2.836 
2022-10-31 08:32:06 [265 - INFO ] Process 200-th batch and its stft mse: 2.713, sisnr: 0.427, lfb mse: 2.214, back loss is : 2.713 
2022-10-31 08:33:30 [265 - INFO ] Process 300-th batch and its stft mse: 2.808, sisnr: -0.126, lfb mse: 2.627, back loss is : 2.808 
2022-10-31 08:34:49 [265 - INFO ] Process 400-th batch and its stft mse: 2.638, sisnr: -0.672, lfb mse: 2.434, back loss is : 2.638 
2022-10-31 08:36:13 [265 - INFO ] Process 500-th batch and its stft mse: 2.610, sisnr: -0.687, lfb mse: 2.548, back loss is : 2.610 
2022-10-31 08:37:35 [265 - INFO ] Process 600-th batch and its stft mse: 2.771, sisnr: 0.114, lfb mse: 3.954, back loss is : 2.771 
2022-10-31 08:39:00 [265 - INFO ] Process 700-th batch and its stft mse: 2.890, sisnr: -0.592, lfb mse: 2.941, back loss is : 2.890 
2022-10-31 08:40:21 [265 - INFO ] Process 800-th batch and its stft mse: 3.028, sisnr: 0.397, lfb mse: 2.305, back loss is : 3.028 
2022-10-31 08:41:41 [265 - INFO ] Process 900-th batch and its stft mse: 3.203, sisnr: -0.103, lfb mse: 2.158, back loss is : 3.203 
2022-10-31 08:43:05 [265 - INFO ] Process 1000-th batch and its stft mse: 2.833, sisnr: 0.370, lfb mse: 2.728, back loss is : 2.833 
2022-10-31 08:44:25 [265 - INFO ] Process 1100-th batch and its stft mse: 2.845, sisnr: -0.833, lfb mse: 2.137, back loss is : 2.845 
2022-10-31 08:45:48 [265 - INFO ] Process 1200-th batch and its stft mse: 3.042, sisnr: 0.516, lfb mse: 2.914, back loss is : 3.042 
2022-10-31 08:47:12 [265 - INFO ] Process 1300-th batch and its stft mse: 2.588, sisnr: -0.330, lfb mse: 2.118, back loss is : 2.588 
2022-10-31 08:48:37 [265 - INFO ] Process 1400-th batch and its stft mse: 2.778, sisnr: -0.116, lfb mse: 2.832, back loss is : 2.778 
2022-10-31 08:49:59 [265 - INFO ] Process 1500-th batch and its stft mse: 3.014, sisnr: -0.497, lfb mse: 2.625, back loss is : 3.014 
2022-10-31 08:50:59 [331 - INFO ] Loss(time/N, lr=1.563e-05) - Epoch 43: train = +2.80782(1264.41s/1531) | dev_loss = +3.65083, (35.54s/4272), sisnr= 1.69291, lfb= 1.57877, stft= 3.65083 | no impr, best = 3.52913
2022-10-31 08:52:29 [265 - INFO ] Process 100-th batch and its stft mse: 3.007, sisnr: -0.335, lfb mse: 2.608, back loss is : 3.007 
2022-10-31 08:53:49 [265 - INFO ] Process 200-th batch and its stft mse: 2.774, sisnr: -0.893, lfb mse: 2.602, back loss is : 2.774 
2022-10-31 08:55:09 [265 - INFO ] Process 300-th batch and its stft mse: 2.770, sisnr: -0.226, lfb mse: 2.243, back loss is : 2.770 
2022-10-31 08:56:32 [265 - INFO ] Process 400-th batch and its stft mse: 3.135, sisnr: -0.690, lfb mse: 2.315, back loss is : 3.135 
2022-10-31 08:57:55 [265 - INFO ] Process 500-th batch and its stft mse: 2.591, sisnr: -0.669, lfb mse: 2.497, back loss is : 2.591 
2022-10-31 08:59:17 [265 - INFO ] Process 600-th batch and its stft mse: 2.900, sisnr: -0.645, lfb mse: 2.279, back loss is : 2.900 
2022-10-31 09:00:39 [265 - INFO ] Process 700-th batch and its stft mse: 2.414, sisnr: -1.159, lfb mse: 2.776, back loss is : 2.414 
2022-10-31 09:02:00 [265 - INFO ] Process 800-th batch and its stft mse: 2.543, sisnr: -1.364, lfb mse: 2.098, back loss is : 2.543 
2022-10-31 09:03:17 [265 - INFO ] Process 900-th batch and its stft mse: 2.593, sisnr: -0.949, lfb mse: 2.309, back loss is : 2.593 
2022-10-31 09:04:38 [265 - INFO ] Process 1000-th batch and its stft mse: 2.992, sisnr: 1.293, lfb mse: 3.656, back loss is : 2.992 
2022-10-31 09:05:58 [265 - INFO ] Process 1100-th batch and its stft mse: 2.870, sisnr: -0.063, lfb mse: 2.974, back loss is : 2.870 
2022-10-31 09:07:21 [265 - INFO ] Process 1200-th batch and its stft mse: 2.933, sisnr: 0.515, lfb mse: 2.213, back loss is : 2.933 
2022-10-31 09:08:41 [265 - INFO ] Process 1300-th batch and its stft mse: 2.826, sisnr: -0.415, lfb mse: 2.106, back loss is : 2.826 
2022-10-31 09:10:04 [265 - INFO ] Process 1400-th batch and its stft mse: 2.442, sisnr: -0.863, lfb mse: 2.244, back loss is : 2.442 
2022-10-31 09:11:25 [265 - INFO ] Process 1500-th batch and its stft mse: 2.691, sisnr: -0.247, lfb mse: 2.340, back loss is : 2.691 
2022-10-31 09:12:25 [331 - INFO ] Loss(time/N, lr=7.813e-06) - Epoch 44: train = +2.80401(1248.56s/1531) | dev_loss = +3.65741, (37.06s/4272), sisnr= 1.71373, lfb= 1.57597, stft= 3.65741 | no impr, best = 3.52913
2022-10-31 09:13:47 [265 - INFO ] Process 100-th batch and its stft mse: 2.602, sisnr: 0.022, lfb mse: 2.831, back loss is : 2.602 
2022-10-31 09:15:07 [265 - INFO ] Process 200-th batch and its stft mse: 2.372, sisnr: -0.676, lfb mse: 2.185, back loss is : 2.372 
2022-10-31 09:16:32 [265 - INFO ] Process 300-th batch and its stft mse: 2.801, sisnr: -0.026, lfb mse: 2.276, back loss is : 2.801 
2022-10-31 09:17:54 [265 - INFO ] Process 400-th batch and its stft mse: 2.921, sisnr: 0.117, lfb mse: 2.899, back loss is : 2.921 
2022-10-31 09:19:19 [265 - INFO ] Process 500-th batch and its stft mse: 2.719, sisnr: -0.713, lfb mse: 2.556, back loss is : 2.719 
2022-10-31 09:20:41 [265 - INFO ] Process 600-th batch and its stft mse: 2.550, sisnr: -1.078, lfb mse: 2.780, back loss is : 2.550 
2022-10-31 09:22:05 [265 - INFO ] Process 700-th batch and its stft mse: 2.986, sisnr: -0.221, lfb mse: 2.716, back loss is : 2.986 
2022-10-31 09:23:20 [265 - INFO ] Process 800-th batch and its stft mse: 2.801, sisnr: -0.652, lfb mse: 2.277, back loss is : 2.801 
2022-10-31 09:24:39 [265 - INFO ] Process 900-th batch and its stft mse: 2.917, sisnr: -0.035, lfb mse: 2.470, back loss is : 2.917 
2022-10-31 09:26:00 [265 - INFO ] Process 1000-th batch and its stft mse: 2.951, sisnr: 0.906, lfb mse: 2.729, back loss is : 2.951 
2022-10-31 09:27:22 [265 - INFO ] Process 1100-th batch and its stft mse: 2.779, sisnr: -1.183, lfb mse: 1.884, back loss is : 2.779 
2022-10-31 09:28:43 [265 - INFO ] Process 1200-th batch and its stft mse: 2.967, sisnr: 0.173, lfb mse: 2.735, back loss is : 2.967 
2022-10-31 09:30:03 [265 - INFO ] Process 1300-th batch and its stft mse: 3.174, sisnr: 0.981, lfb mse: 3.978, back loss is : 3.174 
2022-10-31 09:31:24 [265 - INFO ] Process 1400-th batch and its stft mse: 2.793, sisnr: -0.376, lfb mse: 2.348, back loss is : 2.793 
2022-10-31 09:32:45 [265 - INFO ] Process 1500-th batch and its stft mse: 2.565, sisnr: -0.883, lfb mse: 2.579, back loss is : 2.565 
2022-10-31 09:33:47 [331 - INFO ] Loss(time/N, lr=7.813e-06) - Epoch 45: train = +2.80317(1243.55s/1531) | dev_loss = +3.65460, (37.75s/4272), sisnr= 1.70009, lfb= 1.54658, stft= 3.65460 | no impr, best = 3.52913
2022-10-31 09:35:17 [265 - INFO ] Process 100-th batch and its stft mse: 2.555, sisnr: -0.089, lfb mse: 2.424, back loss is : 2.555 
2022-10-31 09:36:39 [265 - INFO ] Process 200-th batch and its stft mse: 3.175, sisnr: 0.004, lfb mse: 2.495, back loss is : 3.175 
2022-10-31 09:38:02 [265 - INFO ] Process 300-th batch and its stft mse: 3.012, sisnr: -0.115, lfb mse: 2.737, back loss is : 3.012 
2022-10-31 09:39:23 [265 - INFO ] Process 400-th batch and its stft mse: 3.029, sisnr: -0.676, lfb mse: 2.171, back loss is : 3.029 
2022-10-31 09:40:44 [265 - INFO ] Process 500-th batch and its stft mse: 2.721, sisnr: -0.774, lfb mse: 2.298, back loss is : 2.721 
2022-10-31 09:42:05 [265 - INFO ] Process 600-th batch and its stft mse: 2.855, sisnr: -0.452, lfb mse: 2.209, back loss is : 2.855 
2022-10-31 09:43:24 [265 - INFO ] Process 700-th batch and its stft mse: 2.562, sisnr: -0.439, lfb mse: 2.685, back loss is : 2.562 
2022-10-31 09:44:44 [265 - INFO ] Process 800-th batch and its stft mse: 2.789, sisnr: -0.206, lfb mse: 2.619, back loss is : 2.789 
2022-10-31 09:46:03 [265 - INFO ] Process 900-th batch and its stft mse: 2.800, sisnr: -0.681, lfb mse: 2.727, back loss is : 2.800 
2022-10-31 09:47:25 [265 - INFO ] Process 1000-th batch and its stft mse: 2.869, sisnr: -0.533, lfb mse: 2.036, back loss is : 2.869 
2022-10-31 09:48:47 [265 - INFO ] Process 1100-th batch and its stft mse: 2.966, sisnr: -0.422, lfb mse: 2.083, back loss is : 2.966 
2022-10-31 09:50:06 [265 - INFO ] Process 1200-th batch and its stft mse: 2.825, sisnr: 0.709, lfb mse: 2.506, back loss is : 2.825 
2022-10-31 09:51:24 [265 - INFO ] Process 1300-th batch and its stft mse: 2.833, sisnr: -1.053, lfb mse: 2.403, back loss is : 2.833 
2022-10-31 09:52:47 [265 - INFO ] Process 1400-th batch and its stft mse: 2.742, sisnr: -0.794, lfb mse: 2.878, back loss is : 2.742 
2022-10-31 10:01:49 [265 - INFO ] Process 1500-th batch and its stft mse: 2.381, sisnr: -1.302, lfb mse: 2.404, back loss is : 2.381 
2022-10-31 10:05:02 [331 - INFO ] Loss(time/N, lr=7.813e-06) - Epoch 46: train = +2.80322(1787.40s/1531) | dev_loss = +3.66049, (87.71s/4272), sisnr= 1.71568, lfb= 1.60143, stft= 3.66049 | no impr, best = 3.52913
